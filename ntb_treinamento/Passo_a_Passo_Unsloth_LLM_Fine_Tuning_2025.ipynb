{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e5e39c74-bd29-4dd8-b4ee-266b3f2fe783",
      "metadata": {
        "id": "e5e39c74-bd29-4dd8-b4ee-266b3f2fe783"
      },
      "source": [
        "## üëüüëü Passo a passo simples com unsloth: Fine-Tuning de LLMs\n",
        "\n",
        "**Notebook simples**, passo a passo, com a escolha de um modelo, fun√ß√µes de Chat-Template para predi√ß√£o e treinamento, testes de predi√ß√£o antes e depois do treinamento.\n",
        "\n",
        "üìö O objetivo desse notebook √© aprender um pouco mais sobre o fine tuning de LLMs, n√£o √© ser um c√≥digo final de treinamento.\n",
        "\n",
        "Tudo come√ßa com a escolha do modelo e a cria√ß√£o de um bom dataset de treinamento e, nesse exemplo, uma pergunta alvo que espera-se que o modelo aprenda: \"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\"\n",
        "\n",
        "### üîó Inspirado no exemplo de MariyaSha\n",
        "  O que difere, ela fez uma vers√£o bem compacta, n√£o explorou chat template e n√£o se preocupou com pads e tokens especiais de cada modelo, nem m√°scaras de aten√ß√£o, facilitando o entendimento para quem est√° come√ßando.\n",
        "  Essa vers√£o ainda difere por utilizar Unsloth para economizar mem√≥ria e custo computacional.\n",
        "  - Github: https://github.com/MariyaSha/fine_tuning\n",
        "  - V√¨deo: https://youtu.be/uikZs6y0qgI?si=w6rOopXFyh7UxHbM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instalando Unsloth\n",
        "# https://docs.unsloth.ai/get-started/unsloth-notebooks\n",
        "import os, re\n",
        "from IPython.display import clear_output\n",
        "try:\n",
        "  import unsloth\n",
        "  print(\"‚úÖ Unsloth e vllm OK _o/\")\n",
        "  import transformers\n",
        "  print(\"‚úÖ Transformers OK _o/\")\n",
        "except ImportError as e:\n",
        "    clear_output()\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    !pip install transformers==4.55.4\n",
        "    !pip install --no-deps trl==0.22.2\n",
        "    clear_output()\n",
        "    print('‚úÖ Instala√ß√£o do Unsloth e Transformers conclu√≠das _o/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxBW-Ryqb9HB",
        "outputId": "cf7bb68f-4cf4-41dd-a7ec-95fa97f5b086"
      },
      "id": "cxBW-Ryqb9HB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Instala√ß√£o do Unsloth e Transformers conclu√≠das _o/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Escolhendo o modelo e preparando as classes"
      ],
      "metadata": {
        "id": "4lh4MHBGNVyw"
      },
      "id": "4lh4MHBGNVyw"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Escolhendo o modelo\n",
        "# o coment√°rio de acerto est√° relacionado √† pergunta:\n",
        "# - \"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\"\n",
        "model_name = 'meta-llama/Llama-3.2-1B-Instruct' # todo\n",
        "model_name = 'meta-llama/Llama-3.2-3B-Instruct' # todo\n",
        "model_name = 'google/gemma-3-27b-it' # 39Gb     # todo\n",
        "\n",
        "model_name = \"Jurema-br/Jurema-7B\"       # 14Gb PUIL acertou em 50 √©pocas\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"  # PUIL 50 √©pocas - acertou a sigla e errou o resto\n",
        "model_name = 'google/gemma-3-4b-it'      # 9Gb - PUIL errou tudo\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\" # 14Gb PUIL 50 √©pocas acertou a sigla e errou o resto\n",
        "model_name = 'Qwen/Qwen3-8B' # 30Gb PUIL 50 √©pocas acertou sigla e lei\n",
        "model_name ='deepseek-ai/DeepSeek-R1-Distill-Llama-8B' # 21Gb PUIL 50 √©pocas quase a sigla e quase a explica√ß√£o\n",
        "model_name = 'google/gemma-3-1b-it'   # 4.3Gb PUIL 50 √©pocas quase a sigla e quase a explica√ß√£o/ 200 Sigla ok explica√ß√£o quase, lei errada\n",
        "model_name = 'google/gemma-3-12b-it'  # 25Gb PUIL 50 √©pocas errou tudo\n",
        "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' # 7.3Gb PUIL 50 √©pocas errou tudo / 300 continuou errando at√© o ptbr\n",
        "model_name = 'google/gemma-3-4b-it'   # 9Gb - PUIL 50 √©pocas errou tudo\n"
      ],
      "metadata": {
        "id": "ZwLHq7DCjGca"
      },
      "id": "ZwLHq7DCjGca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepara a classe de predi√ß√£o\n",
        "''' Essa classe facilita a carga do modelo para predi√ß√£o antes e depois do treinamento\n",
        "'''\n",
        "print('Importando unsloth e transformers... ')\n",
        "import os\n",
        "import unsloth\n",
        "from unsloth import FastModel\n",
        "from unsloth.chat_templates import get_chat_template, CHAT_TEMPLATES\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "class Modelo:\n",
        "    ''' pode receber:\n",
        "        - o nome do modelo para baixar\n",
        "        - o nome da pasta com os adaptadores\n",
        "        - uma tupla model, tokenizer\n",
        "    '''\n",
        "    def __init__(self, model_name:str, max_seq_length = 2048, n_bits = 8, cache_dir=None):\n",
        "        self.model_name = model_name\n",
        "        if isinstance(model_name, tuple):\n",
        "            # √© um modelo e um tokenizer em mem√≥ria?\n",
        "            self.model = model_name[0]\n",
        "            self.tokenizer = model_name[1]\n",
        "            self.model_name = 'Modelo em mem√≥ria'\n",
        "        else:\n",
        "            # √© um modelo em uma pasta ou o nome no huggingface\n",
        "            bits4 = True if isinstance(n_bits, int) and n_bits == 4 else False\n",
        "            bits8 = True if isinstance(n_bits, int) and n_bits == 8 else False\n",
        "            arq_lora = 'adapter_config.json'\n",
        "            arq_lora = os.path.join(model_name, arq_lora)\n",
        "            self.lora_carregado = os.path.isfile(arq_lora)\n",
        "            self.model, self.tokenizer = FastModel.from_pretrained(\n",
        "                    model_name = model_name,\n",
        "                    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "                    load_in_4bit = bits4,  # 4 bit quantization to reduce memory\n",
        "                    load_in_8bit = bits8, # [NEW!] A bit more accurate, uses 2x memory\n",
        "                    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "                    fast_inference = False,\n",
        "                    device_map      = \"auto\",\n",
        "                    cache_dir       = cache_dir,\n",
        "            )\n",
        "            self._ensure_chat_template()\n",
        "        self.template_com_type(self.tokenizer)\n",
        "        print('=' * 40)\n",
        "        _com_lora = ' com LoRA' if self.lora_carregado else ''\n",
        "        print(f'Modelo \"{self.model_name}\"{_com_lora} carregado!')\n",
        "\n",
        "    def _ensure_chat_template(self):\n",
        "        if getattr(self.tokenizer, \"chat_template\", None):\n",
        "            return  # j√° tem template definido (modelos \"instruct\" costumam trazer)\n",
        "        _nm_teste = self.model_name.replace('-','')\n",
        "        self.message_use_type = False\n",
        "        if 'gemma' in _nm_teste:\n",
        "            key = 'gemma'\n",
        "            self.message_use_type = True\n",
        "        elif 'qwen2' in _nm_teste: key = '\"qwen-2.5\"'\n",
        "        elif 'qwen3' in _nm_teste: key = 'chatml'\n",
        "        elif 'llama33' in _nm_teste:  key = 'llama-3.3'\n",
        "        elif 'llama32' in _nm_teste:  key = 'llama-3.2'\n",
        "        elif 'llama31' in _nm_teste:  key = 'llama-3.1'\n",
        "        elif 'llama3' in _nm_teste:  key = 'llama-3'\n",
        "        elif 'llama' in _nm_teste:  key = 'llama'\n",
        "        else: key = 'chatml'\n",
        "        if key not in CHAT_TEMPLATES:\n",
        "            key = \"chatml\"  # √∫ltimo fallback\n",
        "        self.tokenizer = get_chat_template(self.tokenizer, chat_template=key)\n",
        "\n",
        "    def _place_inputs(self, inputs):\n",
        "        #if self._is_sharded():\n",
        "        #    return inputs#.to(\"cpu\") # deixa o acelerate decidir o device correto\n",
        "        try:\n",
        "            target = self.model.device\n",
        "        except AttributeError:\n",
        "            target = next(self.model.parameters()).device\n",
        "        return inputs.to(target)\n",
        "\n",
        "    @classmethod\n",
        "    def template_com_type(self, tokenizer) -> bool:\n",
        "        msgs_list = [{\"role\": \"user\", \"content\": [{\"type\":\"text\",\"text\":\"ping\"}]}]\n",
        "        msgs_str  = [{\"role\": \"user\", \"content\": \"ping\"}]\n",
        "        try:\n",
        "            tokenizer.apply_chat_template(msgs_list, tokenize=False)\n",
        "            self._msg_type = True\n",
        "            return  True # lista de partes funcionou\n",
        "        except Exception:\n",
        "            pass\n",
        "        # se lista falhou, tente string\n",
        "        tokenizer.apply_chat_template(msgs_str, tokenize=False)  # lan√ßa se n√£o suportar\n",
        "        self._msg_type = False\n",
        "        return False # string funcionou\n",
        "\n",
        "    def resposta(self, prompt):\n",
        "        content = [{\"type\": \"text\", \"text\": prompt}] if self._msg_type else prompt\n",
        "        messages = [{\"role\": \"user\", \"content\": content}]\n",
        "        # Formata a conversa conforme o template e prepara tensores\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,  # requerido para infer√™ncia/chat\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs = self._place_inputs(inputs)\n",
        "        #inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Config de gera√ß√£o (ajuste conforme necessidade)\n",
        "        base_cfg = GenerationConfig(\n",
        "            max_new_tokens=256,\n",
        "            do_sample=False,           # determin√≠stico por padr√£o\n",
        "            temperature=0.0,\n",
        "            top_p=1.0,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "        #if generation_kwargs:\n",
        "        #    base_cfg = GenerationConfig(**{**base_cfg.to_dict(), **generation_kwargs})\n",
        "\n",
        "        # Gera√ß√£o\n",
        "        outputs = self.model.generate(inputs, generation_config=base_cfg)\n",
        "\n",
        "        # Decodifica apenas o trecho gerado (exclui o prompt)\n",
        "        #prompt_len = inputs[\"input_ids\"].shape[-1]\n",
        "        prompt_len = inputs.shape[-1]\n",
        "        generated = outputs[0][prompt_len:]\n",
        "        return self.tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
        "\n",
        "    def print_pergunta_resposta(self, prompt):\n",
        "        print('=' * 50)\n",
        "        print('Pergunta:', prompt)\n",
        "        print('Resposta do modelo:', self.resposta(prompt))\n",
        "        print('-' * 50)\n",
        "\n",
        "print('‚úÖ Tudo ok!')"
      ],
      "metadata": {
        "id": "2zFPp9GiTrl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce89dcf-e619-4b77-fa86-23323e9c229b"
      },
      "id": "2zFPp9GiTrl1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importando unsloth e transformers... \n",
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "‚úÖ Tudo ok!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d105dd4f-c6b4-4409-b43c-d9be42df4ffc",
      "metadata": {
        "id": "d105dd4f-c6b4-4409-b43c-d9be42df4ffc"
      },
      "outputs": [],
      "source": [
        "#@title Carga e predi√ß√£o do modelo base\n",
        "\n",
        "# ATEN√á√ÉO: se tiver pouca mem√≥ria de GPU, reinicie a sess√£o e pule essa c√©lula para\n",
        "#          realizar o treinamento com a GPU livre\n",
        "\n",
        "md = Modelo(model_name)\n",
        "md.print_pergunta_resposta(\"O que √© a classe processual PUIL?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2cf51a-85c9-45bc-8130-a14573db3fc0",
      "metadata": {
        "id": "0b2cf51a-85c9-45bc-8130-a14573db3fc0"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Precisamos ensinar o modelo com prompts e suas respectivas respostas. Podemos usar um dataset padr√£o para a entrada, mas cada modelo tem um formato pr√≥prio que pode ser formadado com o chat-template do modelo:\n",
        "\n",
        "### Formato padr√£o de entrada\n",
        "- JsonL Um json por linha\n",
        "```\n",
        "{\"prompt\": \"O que √© PUIL?\", \"completion\": \"Uma classe processual do STJ\"}\n",
        "{\"prompt\": \"O que significa PUIL\", \"completion\": \"Pedido de Uniformiza√ß√£o de Interpreta√ß√£o de Lei\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "391dcfd9-86ff-4686-b947-c06ce2d35a74",
      "metadata": {
        "id": "391dcfd9-86ff-4686-b947-c06ce2d35a74"
      },
      "outputs": [],
      "source": [
        "#@title Passo 1: carregando dados para o treino\n",
        "from datasets import load_dataset\n",
        "arq = \"puil_treinamento.txt\"\n",
        "#carrega do git\n",
        "if not os.path.isfile(arq):\n",
        "   ! curl -o {arq} https://raw.githubusercontent.com/luizanisio/llms/refs/heads/main/ntb_treinamento/puil_treinamento.txt\n",
        "raw_data = load_dataset(\"json\", data_files=arq)\n",
        "print('Dataset carregado:', raw_data)\n",
        "\n",
        "print('Exemplo:', raw_data[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Passo 2 processando dados com chat template\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset # Exemplo, caso precise carregar um dataset\n",
        "\n",
        "''' Alguns modelos de instru√ß√£o n√£o t√™m um pad_token, se for o caso o pad_token\n",
        "    ser√° configurado para usar o eos_token.\n",
        "    Para otimizar o SFT e apontar os mecanismos de aten√ß√£o apenas para a predi√ß√£o,\n",
        "    os labels e m√°scaras de aten√ß√£o v√£o ser configurados para que o treinamento\n",
        "    n√£o perca tempo com os tokens do prompt, epenas aprendam com a resposta.\n",
        "    Aqui faz-se um workaround para identificar exatamente os tokens que ser√£o aprendidos\n",
        "    ao aplicar o chat template sem a resposta e com a resposta.\n",
        "    Poderiam ser identificados os tokens especiais de cada modelo, mas perder√≠amos a\n",
        "    chance de utilizar o chat_template pr√≥prio que j√° absorve essa intelig√™ncia para cada\n",
        "    modelo.\n",
        "'''\n",
        "print('Preparando dataset com o modelo:', model_name)\n",
        "\n",
        "train_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "if train_tokenizer.pad_token is None:\n",
        "    print('ATEN√á√ÉO: O modelo n√£o tinha pad_token e foi utilizado o eos_token')\n",
        "    train_tokenizer.pad_token = train_tokenizer.eos_token\n",
        "usar_type = Modelo.template_com_type(train_tokenizer)\n",
        "\n",
        "def preprocess_with_chat_template(sample, max_length=1024):\n",
        "    \"\"\"\n",
        "    Mascara tudo que vem antes do in√≠cio da resposta do assistant e todo padding.\n",
        "    Compat√≠vel com chat template do Gemma (sem procurar tags em string).\n",
        "    \"\"\"\n",
        "    # 1) IDs at√© o ponto em que o modelo come√ßaria a responder (sem completion):\n",
        "    content_usr = [{\"type\": \"text\", \"text\": sample[\"prompt\"]}] if usar_type else sample[\"prompt\"]\n",
        "    content_ast = [{\"type\": \"text\", \"text\": sample[\"completion\"]}] if usar_type else sample[\"completion\"]\n",
        "    messages_prompt = [{\"role\": \"user\", \"content\": content_usr},]\n",
        "    prompt_ids = train_tokenizer.apply_chat_template(\n",
        "        messages_prompt,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,   # adiciona o cabe√ßalho do assistant/model\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # 2) IDs da conversa completa (com a resposta do assistant):\n",
        "    messages_full = [\n",
        "        {\"role\": \"user\", \"content\": content_usr},\n",
        "        {\"role\": \"assistant\", \"content\": content_ast}, ]\n",
        "\n",
        "    full_ids = train_tokenizer.apply_chat_template(\n",
        "        messages_full,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # 3) Truncar/padding de forma consistente\n",
        "    # Observa√ß√£o: truncando manualmente para preservar o cutoff corretamente.\n",
        "    input_ids = full_ids[:max_length]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "    if len(input_ids) < max_length:\n",
        "        pad_len = max_length - len(input_ids)\n",
        "        input_ids = input_ids + [train_tokenizer.pad_token_id] * pad_len\n",
        "        attention_mask = attention_mask + [0] * pad_len\n",
        "\n",
        "    # 4) Cutoff = in√≠cio da resposta do assistant (limitado pelo max_length)\n",
        "    cutoff = min(len(prompt_ids), max_length)\n",
        "\n",
        "    # 5) Labels = c√≥pia de input_ids; m√°scara em [0:cutoff) e em padding\n",
        "    labels = input_ids.copy()\n",
        "    for j in range(max_length):\n",
        "        if j < cutoff or attention_mask[j] == 0:\n",
        "            labels[j] = -100 # labels para serem ignorados no treinamento\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "def calcular_qtd_tokens(sample):\n",
        "    messages_full = [\n",
        "        {\"role\": \"user\", \"content\": sample[\"prompt\"]},\n",
        "        {\"role\": \"assistant\", \"content\": sample[\"completion\"]},]\n",
        "    full_ids = train_tokenizer.apply_chat_template(\n",
        "        messages_full,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    return {'qtd_tokens':len(full_ids)}\n",
        "\n",
        "print('Colunas originais:',raw_data.column_names)\n",
        "print(raw_data.column_names['train'][0])\n",
        "# --- Processando o Dataset ---\n",
        "# Inicialmente vamos calcular o n√∫mero m√°ximo de de tokens que o dataset exige\n",
        "qtd_tokens_dataset = raw_data.map(\n",
        "    calcular_qtd_tokens,\n",
        "    remove_columns=raw_data.column_names['train']\n",
        ")\n",
        "min_tokens = min(qtd_tokens_dataset['train']['qtd_tokens'])\n",
        "max_tokens = max(qtd_tokens_dataset['train']['qtd_tokens'])\n",
        "print('M√≠nimo de tokens identificados no dataset: ', min_tokens)\n",
        "print('M√°ximo de tokens identificados no dataset: ', max_tokens)\n",
        "# Aplicamos a nova fun√ß√£o ao seu conjunto de dados.\n",
        "# `remove_columns` √© √∫til para limpar as colunas de texto originais que n√£o s√£o mais necess√°rias.\n",
        "data = raw_data.map(\n",
        "    lambda x: preprocess_with_chat_template(x, max_length=max_tokens),\n",
        "    remove_columns=raw_data.column_names['train']\n",
        ")\n",
        "print('Colunas treino:', data.column_names)\n",
        "\n",
        "i_teste = 0\n",
        "# Vamos decodificar uma amostra para ver como ficou\n",
        "print(\"Um exemplo processado:\")\n",
        "print(data['train'][i_teste])\n",
        "print(\"\\nAmostra decodificada:\")\n",
        "print(train_tokenizer.decode(data['train'][i_teste]['input_ids']))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oUcUrjb_5pip"
      },
      "id": "oUcUrjb_5pip",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quais m√≥dulos do modelo podem ser treinados\n",
        "''' O treinamento pode impactar v√°rios m√≥dulos do modelo,\n",
        "    para os exemplos mais comuns, usa-se os m√≥dulos ['k_proj','q_proj', 'v_proj']\n",
        "    mas pode-se usar todos os m√≥dulos lineares.\n",
        "'''\n",
        "\n",
        "def modulos_treino(model):\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        # A camada que queremos treinar pode ser uma camada linear padr√£o (torch.nn.Linear)\n",
        "        # ou uma camada linear quantizada (como a do bitsandbytes).\n",
        "        # O ideal √© verificar por um nome de classe que abranja ambos, como 'Linear'.\n",
        "        if 'Linear' in str(type(module)):\n",
        "            # Os nomes dos m√≥dulos LoRA s√£o geralmente os √∫ltimos componentes do nome completo.\n",
        "            # Ex: \"model.layers.0.self_attn.q_proj\" -> queremos \"q_proj\"\n",
        "            module_name = name.split('.')[-1]\n",
        "            lora_module_names.add(module_name)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"M√≥dulos lineares encontrados que podem ser usados como `target_modules` para LoRA:\")\n",
        "    print(lora_module_names)\n",
        "    print(\"=\"*50)\n",
        "    return lora_module_names\n"
      ],
      "metadata": {
        "id": "mV6C2PhOLNKe"
      },
      "id": "mV6C2PhOLNKe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Passo 3 preparando LoRa\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "\n",
        "print('MODEL NAME:', md.model_name)\n",
        "print('Tipo do modelo antes de aplicar o Peft', type(md.model))\n",
        "\n",
        "target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "\n",
        "train_model = FastLanguageModel.get_peft_model(\n",
        "    md.model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "\n",
        "print('Tipo do modelo depois de aplicar o Peft', type(train_model))\n",
        "print('√â um modelo Peft?', isinstance(train_model, PeftModel))\n",
        "print('LoRA preparado para treinar os m√≥dulos:', target_modules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC_Me4432D0V",
        "outputId": "43d96493-bbdb-48d9-e8d2-b753daff1892"
      },
      "id": "NC_Me4432D0V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL NAME: google/gemma-3-4b-it\n",
            "Tipo do modelo antes de aplicar o Peft <class 'transformers.models.gemma3.modeling_gemma3.Gemma3ForConditionalGeneration'>\n",
            "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n",
            "Tipo do modelo depois de aplicar o Peft <class 'peft.peft_model.PeftModelForCausalLM'>\n",
            "√â um modelo Peft? True\n",
            "LoRA preparado para treinar os m√≥dulos: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e6bfce-f810-4806-ab65-d46f3c77e2e8",
      "metadata": {
        "id": "37e6bfce-f810-4806-ab65-d46f3c77e2e8"
      },
      "source": [
        "## 4 e 5 - Treinamento propriamente dito\n",
        "\n",
        "- defina o n√∫mero de √©pocas, decaimento do learning rate, aquecimento do learning rate, localiza√ß√£o dos logs de treinamento, etc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Passo 4 preparando um treino simples\n",
        "# https://huggingface.co/docs/trl/en/sft_trainer\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "EPOCAS = 50\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = train_model,              # modelo j√° carregado (HF/Unsloth/PEFT)\n",
        "#   tokenizer = tokenizer,            # desnecess√°rio pois o dataset j√© retorna os n√∫meros dos tokens\n",
        "    train_dataset = data[\"train\"],    # dataset de treino com o campo de texto abaixo\n",
        "    eval_dataset  = None,             # defina um dataset de valida√ß√£o p/ m√©tricas peri√≥dicas\n",
        "    args = SFTConfig(\n",
        "        num_train_epochs = EPOCAS,    # epochs de treino\n",
        "        dataset_text_field = \"text\",  # coluna do dataset que cont√©m o texto de entrada\n",
        "        per_device_train_batch_size = 2,   # batch por GPU/TPU (antes de acumula√ß√£o)\n",
        "        gradient_accumulation_steps = 4,   # n¬∫ de passos acumulados antes do otimizador dar step\n",
        "                                           # ‚áí batch_efetivo ‚âà batch_size * acum_steps * n¬∫_dispositivos\n",
        "        warmup_steps = 5,             # passos iniciais de aquecimento do LR (cresce do 0 ao LR alvo)\n",
        "        learning_rate = 2e-4,         # taxa inicial; use menor (ex.: 2e-5) p/ treinos longos/est√°veis\n",
        "        optim = \"adamw_8bit\",         # AdamW com estados em 8-bit (economia de VRAM, finetuning de LLMs)\n",
        "        weight_decay = 0.01,          # regulariza√ß√£o L2 nos pesos (evita overfitting em cabe√ßas densas)\n",
        "        lr_scheduler_type = \"linear\", # LR decai linearmente ap√≥s warmup (padr√£o no HF Trainer)\n",
        "        seed = 3407,                  # reprodutibilidade (shuffles, inicializa√ß√£o, etc.)\n",
        "        report_to = \"none\",           # \"wandb\", \"tensorboard\", etc. p/ enviar m√©tricas\n",
        "        logging_steps=25,\n",
        "        logging_dir='./log',\n",
        "        output_dir='./results' # Output directory for saving checkpoints and logs\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "jLJeRs6Ix2r8"
      },
      "id": "jLJeRs6Ix2r8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Passo 4.1 treinando\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "7Qu5DrQmyZt1"
      },
      "id": "7Qu5DrQmyZt1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Passo 5 testando o modelo treinado em mem√≥ria\n",
        "\n",
        "#md = Modelo((train_model, train_tokenizer))\n",
        "md.print_pergunta_resposta(\"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgBzBXBF_Z2Y",
        "outputId": "09e78647-aae2-4ced-e9e3-6fd64599aeff"
      },
      "id": "TgBzBXBF_Z2Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Pergunta: O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\n",
            "Resposta do modelo: √â o Pedido de Uniformiza√ß√£o de Interpreta√ß√£o de Lei; a lei √© a TNU (art. 14).\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb33a27-3b70-4176-8660-7c25131729af",
      "metadata": {
        "id": "9bb33a27-3b70-4176-8660-7c25131729af"
      },
      "source": [
        "# Gravando o modelo no disco\n",
        "- voc√™ precisa salvar o modelo e o tokenizer juntos para serem carregados no futuro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90574f6-bd50-4bf5-a574-f0535b2ab5f8",
      "metadata": {
        "id": "f90574f6-bd50-4bf5-a574-f0535b2ab5f8"
      },
      "outputs": [],
      "source": [
        "# s√≥ os pesos do LoRA\n",
        "print('Gravando pesos do LoRA')\n",
        "train_model.save_pretrained(\"model_lora\")  # Local saving\n",
        "train_tokenizer.save_pretrained(\"model_lora\")\n",
        "\n",
        "print('Gravando merge 16bit')\n",
        "train_model.save_pretrained_merged(\"model_merged\", train_tokenizer, save_method = \"merged_16bit\",)\n",
        "\n",
        "print('Gravando base e LoRA')\n",
        "# modelo e LoRA\n",
        "trainer.save_model(\"./model\")\n",
        "train_tokenizer.save_pretrained(\"./model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60029921-47ee-4682-b6a7-8e5107c3a9ae",
      "metadata": {
        "id": "60029921-47ee-4682-b6a7-8e5107c3a9ae"
      },
      "source": [
        "## Carregando o modelo do disco para teste\n",
        "- Nesse ponto pode ser interessante reiniciar a sess√£o do notebook para liberar mem√≥ria e carregar apenas o modelo treinado\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "md = Modelo('./model_merged')\n",
        "md.print_pergunta_resposta(\"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\")"
      ],
      "metadata": {
        "id": "hbpPwdttQtjb"
      },
      "id": "hbpPwdttQtjb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Carregando do disco e testando o modelo base\n",
        "md = Modelo(model_name)\n",
        "md.print_pergunta_resposta(\"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\")"
      ],
      "metadata": {
        "id": "QZPnCcg3eIzm"
      },
      "id": "QZPnCcg3eIzm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}