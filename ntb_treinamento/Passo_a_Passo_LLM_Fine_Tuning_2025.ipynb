{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e5e39c74-bd29-4dd8-b4ee-266b3f2fe783",
      "metadata": {
        "id": "e5e39c74-bd29-4dd8-b4ee-266b3f2fe783"
      },
      "source": [
        "## üëüüëü Passo a passo simples: Fine-Tuning de LLMs\n",
        "\n",
        "**Notebook simples**, passo a passo, com a escolha de um modelo, fun√ß√µes de Chat-Template para predi√ß√£o e treinamento, testes de predi√ß√£o antes e depois do treinamento.\n",
        "\n",
        "üìö O objetivo desse notebook √© aprender um pouco mais sobre o fine tuning de LLMs, n√£o √© ser um c√≥digo final de treinamento.\n",
        "\n",
        "Tudo come√ßa com a escolha do modelo e a cria√ß√£o de um bom dataset de treinamento e, nesse exemplo, uma pergunta alvo que espera-se que o modelo aprenda: \"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\"\n",
        "\n",
        "### üîó Inspirado no exemplo de MariyaSha\n",
        "  O que difere, ela fez uma vers√£o bem compacta, n√£o explorou chat template e n√£o se preocupou com pads e tokens especiais de cada modelo, nem m√°scaras de aten√ß√£o, facilitando o entendimento para quem est√° come√ßando.\n",
        "  - Github: https://github.com/MariyaSha/fine_tuning\n",
        "  - V√¨deo: https://youtu.be/uikZs6y0qgI?si=w6rOopXFyh7UxHbM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Escolhendo o modelo e preparando as classes"
      ],
      "metadata": {
        "id": "4lh4MHBGNVyw"
      },
      "id": "4lh4MHBGNVyw"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Escolhendo o modelo\n",
        "# o coment√°rio de acerto est√° relacionado √† pergunta:\n",
        "# - \"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\"\n",
        "model_name = 'meta-llama/Llama-3.2-1B-Instruct' # todo\n",
        "model_name = 'meta-llama/Llama-3.2-3B-Instruct' # todo\n",
        "model_name = 'google/gemma-3-27b-it' # 39Gb     # todo\n",
        "\n",
        "model_name = \"Jurema-br/Jurema-7B\"       # 14Gb PUIL acertou em 50 √©pocas\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"  # PUIL 50 √©pocas - acertou a sigla e errou o resto\n",
        "model_name = 'google/gemma-3-4b-it'      # 9Gb - PUIL errou tudo\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\" # 14Gb PUIL 50 √©pocas acertou a sigla e errou o resto\n",
        "model_name = 'Qwen/Qwen3-8B' # 30Gb PUIL 50 √©pocas acertou sigla e lei\n",
        "model_name ='deepseek-ai/DeepSeek-R1-Distill-Llama-8B' # 21Gb PUIL 50 √©pocas quase a sigla e quase a explica√ß√£o\n",
        "model_name = 'google/gemma-3-4b-it'   # 9Gb - PUIL 50 √©pocas errou tudo\n",
        "model_name = 'google/gemma-3-1b-it'   # 4.3Gb PUIL 50 √©pocas quase a sigla e quase a explica√ß√£o/ 200 Sigla ok explica√ß√£o quase, lei errada\n",
        "model_name = 'google/gemma-3-12b-it'  # 25Gb PUIL 50 √©pocas errou tudo\n",
        "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' # 7.3Gb PUIL 50 √©pocas errou tudo / 300 continuou errando at√© o ptbr\n"
      ],
      "metadata": {
        "id": "ZwLHq7DCjGca"
      },
      "id": "ZwLHq7DCjGca",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepara a classe de predi√ß√£o\n",
        "''' Essa classe facilita a carga do modelo para predi√ß√£o antes e depois do treinamento\n",
        "'''\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "class Modelo:\n",
        "    ''' pode receber:\n",
        "        - o nome do modelo para baixar\n",
        "        - o nome da pasta com os adaptadores\n",
        "        - uma tupla model, tokenizer\n",
        "    '''\n",
        "    def __init__(self, model_name:str):\n",
        "        arq_lora = 'adapter_config.json'\n",
        "        if isinstance(model_name, str):\n",
        "           arq_lora = os.path.join(model_name, arq_lora)\n",
        "        self.model_name = model_name\n",
        "        self.lora_carregado = False\n",
        "        if isinstance(model_name, tuple):\n",
        "            # √© um modelo e um tokenizer em mem√≥ria?\n",
        "            self.pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model= model_name[0],\n",
        "                tokenizer = model_name[1]\n",
        "            )\n",
        "            self.model_name = 'Modelo em mem√≥ria'\n",
        "        elif os.path.isfile(arq_lora):\n",
        "            # √© um modelo j√° treinado com adaptadores dispon√≠veis?\n",
        "            lora_config = PeftConfig.from_pretrained(model_name)\n",
        "            modelo_base = AutoModelForCausalLM.from_pretrained(\n",
        "                lora_config.base_model_name_or_path,\n",
        "                torch_dtype=torch.float16, # Match dtype with training\n",
        "                device_map=\"auto\", # Use auto device map\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            peft_model = PeftModel.from_pretrained(modelo_base, model_name)\n",
        "            loaded_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "            merged_model = peft_model.merge_and_unload()\n",
        "            self.pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model= model_name,\n",
        "                tokenizer = loaded_tokenizer\n",
        "            )\n",
        "            self.lora_carregado = True\n",
        "        else:\n",
        "            # √© um modelo em uma pasta ou o nome no huggingface\n",
        "            self.pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model= model_name,\n",
        "                device=\"cuda\"\n",
        "            )\n",
        "        print('=' * 40)\n",
        "        _com_lora = ' com LoRA' if self.lora_carregado else ''\n",
        "        print(f'Modelo \"{self.model_name}\"{_com_lora} carregado!')\n",
        "\n",
        "    def resposta(self, prompt):\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        return self.pipeline(messages, return_full_text=False)[0]['generated_text']\n",
        "\n",
        "    def print_pergunta_resposta(self, prompt):\n",
        "        print('=' * 50)\n",
        "        print('Pergunta:', prompt)\n",
        "        print('Resposta do modelo:', self.resposta(prompt))\n",
        "        print('-' * 50)"
      ],
      "metadata": {
        "id": "2zFPp9GiTrl1"
      },
      "id": "2zFPp9GiTrl1",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d105dd4f-c6b4-4409-b43c-d9be42df4ffc",
      "metadata": {
        "id": "d105dd4f-c6b4-4409-b43c-d9be42df4ffc"
      },
      "outputs": [],
      "source": [
        "#@title Carga e predi√ß√£o do modelo base\n",
        "\n",
        "# ATEN√á√ÉO: se tiver pouca mem√≥ria de GPU, reinicie a sess√£o e pule essa c√©lula para\n",
        "#          realizar o treinamento com a GPU livre\n",
        "\n",
        "md = Modelo(model_name)\n",
        "md.print_pergunta_resposta(\"O que √© a classe processual PUIL?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2cf51a-85c9-45bc-8130-a14573db3fc0",
      "metadata": {
        "id": "0b2cf51a-85c9-45bc-8130-a14573db3fc0"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Precisamos ensinar o modelo com prompts e suas respectivas respostas. Podemos usar um dataset padr√£o para a entrada, mas cada modelo tem um formato pr√≥prio que pode ser formadado com o chat-template do modelo:\n",
        "\n",
        "### Formato padr√£o de entrada\n",
        "- JsonL Um json por linha\n",
        "```\n",
        "{\"prompt\": \"O que √© PUIL?\", \"completion\": \"Uma classe processual do STJ\"}\n",
        "{\"prompt\": \"O que significa PUIL\", \"completion\": \"Pedido de Uniformiza√ß√£o de Interpreta√ß√£o de Lei\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "391dcfd9-86ff-4686-b947-c06ce2d35a74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "391dcfd9-86ff-4686-b947-c06ce2d35a74",
        "outputId": "4f5ed0c4-4143-4d10-8a7f-26276746fc65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset carregado: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['prompt', 'completion'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "})\n",
            "Exemplo: {'prompt': 'O que √© PUIL?', 'completion': '√â o Pedido de Uniformiza√ß√£o de Interpreta√ß√£o de Lei, utilizado para resolver diverg√™ncia na interpreta√ß√£o de lei federal entre Turmas Recursais dos Juizados Especiais Federais (JEFs).'}\n"
          ]
        }
      ],
      "source": [
        "#@title Passo 1: carregando dados para o treino\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_data = load_dataset(\"json\", data_files=\"puil_treinamento.txt\")\n",
        "print('Dataset carregado:', raw_data)\n",
        "\n",
        "print('Exemplo:', raw_data[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Passo 2 processando dados com chat template\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset # Exemplo, caso precise carregar um dataset\n",
        "\n",
        "''' Alguns modelos de instru√ß√£o n√£o t√™m um pad_token, se for o caso o pad_token\n",
        "    ser√° configurado para usar o eos_token.\n",
        "    Para otimizar o SFT e apontar os mecanismos de aten√ß√£o apenas para a predi√ß√£o,\n",
        "    os labels e m√°scaras de aten√ß√£o v√£o ser configurados para que o treinamento\n",
        "    n√£o perca tempo com os tokens do prompt, epenas aprendam com a resposta.\n",
        "    Aqui faz-se um workaround para identificar exatamente os tokens que ser√£o aprendidos\n",
        "    ao aplicar o chat template sem a resposta e com a resposta.\n",
        "    Poderiam ser identificados os tokens especiais de cada modelo, mas perder√≠amos a\n",
        "    chance de utilizar o chat_template pr√≥prio que j√° absorve essa intelig√™ncia para cada\n",
        "    modelo.\n",
        "'''\n",
        "print('Preparando dataset com o modelo:', model_name)\n",
        "\n",
        "train_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "if train_tokenizer.pad_token is None:\n",
        "    print('ATEN√á√ÉO: O modelo n√£o tinha pad_token e foi utilizado o eos_token')\n",
        "    train_tokenizer.pad_token = train_tokenizer.eos_token\n",
        "\n",
        "def preprocess_with_chat_template(sample, max_length=1024):\n",
        "    \"\"\"\n",
        "    Mascara tudo que vem antes do in√≠cio da resposta do assistant e todo padding.\n",
        "    Compat√≠vel com chat template do Gemma (sem procurar tags em string).\n",
        "    \"\"\"\n",
        "    # 1) IDs at√© o ponto em que o modelo come√ßaria a responder (sem completion):\n",
        "    messages_prompt = [{\"role\": \"user\", \"content\": sample[\"prompt\"]},]\n",
        "    prompt_ids = train_tokenizer.apply_chat_template(\n",
        "        messages_prompt,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,   # adiciona o cabe√ßalho do assistant/model\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # 2) IDs da conversa completa (com a resposta do assistant):\n",
        "    messages_full = [\n",
        "        {\"role\": \"user\", \"content\": sample[\"prompt\"]},\n",
        "        {\"role\": \"assistant\", \"content\": sample[\"completion\"]}, ]\n",
        "\n",
        "    full_ids = train_tokenizer.apply_chat_template(\n",
        "        messages_full,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # 3) Truncar/padding de forma consistente\n",
        "    # Observa√ß√£o: truncando manualmente para preservar o cutoff corretamente.\n",
        "    input_ids = full_ids[:max_length]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "    if len(input_ids) < max_length:\n",
        "        pad_len = max_length - len(input_ids)\n",
        "        input_ids = input_ids + [train_tokenizer.pad_token_id] * pad_len\n",
        "        attention_mask = attention_mask + [0] * pad_len\n",
        "\n",
        "    # 4) Cutoff = in√≠cio da resposta do assistant (limitado pelo max_length)\n",
        "    cutoff = min(len(prompt_ids), max_length)\n",
        "\n",
        "    # 5) Labels = c√≥pia de input_ids; m√°scara em [0:cutoff) e em padding\n",
        "    labels = input_ids.copy()\n",
        "    for j in range(max_length):\n",
        "        if j < cutoff or attention_mask[j] == 0:\n",
        "            labels[j] = -100 # labels para serem ignorados no treinamento\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "def calcular_qtd_tokens(sample):\n",
        "    messages_full = [\n",
        "        {\"role\": \"user\", \"content\": sample[\"prompt\"]},\n",
        "        {\"role\": \"assistant\", \"content\": sample[\"completion\"]},]\n",
        "    full_ids = train_tokenizer.apply_chat_template(\n",
        "        messages_full,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    return {'qtd_tokens':len(full_ids)}\n",
        "\n",
        "print('Colunas originais:',raw_data.column_names)\n",
        "print(raw_data.column_names['train'][0])\n",
        "# --- Processando o Dataset ---\n",
        "# Inicialmente vamos calcular o n√∫mero m√°ximo de de tokens que o dataset exige\n",
        "qtd_tokens_dataset = raw_data.map(\n",
        "    calcular_qtd_tokens,\n",
        "    remove_columns=raw_data.column_names['train']\n",
        ")\n",
        "min_tokens = min(qtd_tokens_dataset['train']['qtd_tokens'])\n",
        "max_tokens = max(qtd_tokens_dataset['train']['qtd_tokens'])\n",
        "print('M√≠nimo de tokens identificados no dataset: ', min_tokens)\n",
        "print('M√°ximo de tokens identificados no dataset: ', max_tokens)\n",
        "# Aplicamos a nova fun√ß√£o ao seu conjunto de dados.\n",
        "# `remove_columns` √© √∫til para limpar as colunas de texto originais que n√£o s√£o mais necess√°rias.\n",
        "data = raw_data.map(\n",
        "    lambda x: preprocess_with_chat_template(x, max_length=max_tokens),\n",
        "    remove_columns=raw_data.column_names['train']\n",
        ")\n",
        "print('Colunas treino:', data.column_names)\n",
        "\n",
        "i_teste = 0\n",
        "# Vamos decodificar uma amostra para ver como ficou\n",
        "print(\"Um exemplo processado:\")\n",
        "print(data['train'][i_teste])\n",
        "print(\"\\nAmostra decodificada:\")\n",
        "print(train_tokenizer.decode(data['train'][i_teste]['input_ids']))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oUcUrjb_5pip"
      },
      "id": "oUcUrjb_5pip",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quais m√≥dulos do modelo podem ser treinados\n",
        "''' O treinamento pode impactar v√°rios m√≥dulos do modelo,\n",
        "    para os exemplos mais comuns, usa-se os m√≥dulos ['k_proj','q_proj', 'v_proj']\n",
        "    mas pode-se usar todos os m√≥dulos lineares.\n",
        "'''\n",
        "\n",
        "def modulos_treino(model):\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        # A camada que queremos treinar pode ser uma camada linear padr√£o (torch.nn.Linear)\n",
        "        # ou uma camada linear quantizada (como a do bitsandbytes).\n",
        "        # O ideal √© verificar por um nome de classe que abranja ambos, como 'Linear'.\n",
        "        if 'Linear' in str(type(module)):\n",
        "            # Os nomes dos m√≥dulos LoRA s√£o geralmente os √∫ltimos componentes do nome completo.\n",
        "            # Ex: \"model.layers.0.self_attn.q_proj\" -> queremos \"q_proj\"\n",
        "            module_name = name.split('.')[-1]\n",
        "            lora_module_names.add(module_name)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"M√≥dulos lineares encontrados que podem ser usados como `target_modules` para LoRA:\")\n",
        "    print(lora_module_names)\n",
        "    print(\"=\"*50)\n",
        "    return lora_module_names\n"
      ],
      "metadata": {
        "id": "mV6C2PhOLNKe"
      },
      "id": "mV6C2PhOLNKe",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d23b478a-c89f-459e-9144-2cb10a7b48f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d23b478a-c89f-459e-9144-2cb10a7b48f6",
        "outputId": "aba1ecb4-32f2-42af-c420-56c0fa94e6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL NAME: google/gemma-3-1b-it\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "M√≥dulos lineares encontrados que podem ser usados como `target_modules` para LoRA:\n",
            "{'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'down_proj', 'o_proj', 'k_proj', 'lm_head'}\n",
            "==================================================\n",
            "LoRA preparado para treinar os m√≥dulos: ['q_proj', 'k_proj', 'v_proj']\n"
          ]
        }
      ],
      "source": [
        "#@title Passo 3 preparando um treino simples\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "print('MODEL NAME:', model_name)\n",
        "\n",
        "train_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"cuda\",\n",
        "    torch_dtype = torch.float16\n",
        ")\n",
        "train_tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                                trust_remote_code=True,\n",
        "                                                attn_implementation='eager')\n",
        "\n",
        "modulos_treino(train_model)\n",
        "\n",
        "# escolha os m√≥dulos que ser√£o treinados\n",
        "#target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        "    target_modules = target_modules,\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        ")\n",
        "\n",
        "train_model = get_peft_model(train_model, lora_config)\n",
        "\n",
        "print('LoRA preparado para treinar os m√≥dulos:', target_modules)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e6bfce-f810-4806-ab65-d46f3c77e2e8",
      "metadata": {
        "id": "37e6bfce-f810-4806-ab65-d46f3c77e2e8"
      },
      "source": [
        "## 4 e 5 - Treinamento propriamente dito\n",
        "\n",
        "- defina o n√∫mero de √©pocas, decaimento do learning rate, aquecimento do learning rate, localiza√ß√£o dos logs de treinamento, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60bd0902-e9b2-4b08-89d4-161debf0709f",
      "metadata": {
        "id": "60bd0902-e9b2-4b08-89d4-161debf0709f"
      },
      "outputs": [],
      "source": [
        "#@title Passo 4 - treinando - rode v√°rias vezes se quiser ou aumente o n√∫mero de √©pocas\n",
        "from transformers import TrainingArguments, Trainer, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "EPOCAS = 50\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=EPOCAS,\n",
        "    learning_rate=2e-4,  # Starting learning rate\n",
        "    weight_decay=0.01,\n",
        "    fp16=True, # Se sua GPU suportar, acelera muito o treino\n",
        "    lr_scheduler_type='cosine', # cosine ou linear\n",
        "    warmup_ratio=0.05, # Warmup over the first 5% of training steps\n",
        "    logging_steps=25,\n",
        "    logging_dir='./log',\n",
        "    report_to='none', # Disable reporting to wandb\n",
        "    output_dir='./results' # Output directory for saving checkpoints and logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=train_model,\n",
        "    args=training_args,\n",
        "    train_dataset=data[\"train\"]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Passo 5 testando o modelo treinado em mem√≥ria\n",
        "\n",
        "md = Modelo((train_model, train_tokenizer))\n",
        "md.print_pergunta_resposta(\"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgBzBXBF_Z2Y",
        "outputId": "58d7be1c-b310-414b-f7f9-d5314aa5f1c2"
      },
      "id": "TgBzBXBF_Z2Y",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "Modelo \"Modelo em mem√≥ria\" carregado!\n",
            "==================================================\n",
            "Pergunta: O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\n",
            "Resposta do modelo: √â o Pedido de Uniformiza√ß√£o de Interpreta√ß√£o de Lei; √© instaurado para uniformizar a interpreta√ß√£o de lei federal entre Turmas Recursais dos Juizados Especiais Federais (PUIL) e pretende ser substitu√≠do pelo Art. 14 da Lei 9.027/1995.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb33a27-3b70-4176-8660-7c25131729af",
      "metadata": {
        "id": "9bb33a27-3b70-4176-8660-7c25131729af"
      },
      "source": [
        "# Gravando o modelo no disco\n",
        "- voc√™ precisa salvar o modelo e o tokenizer juntos para serem carregados no futuro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f90574f6-bd50-4bf5-a574-f0535b2ab5f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f90574f6-bd50-4bf5-a574-f0535b2ab5f8",
        "outputId": "91bd2cdb-915c-40da-fa27-2fd91608baf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./my_model/tokenizer_config.json',\n",
              " './my_model/special_tokens_map.json',\n",
              " './my_model/chat_template.jinja',\n",
              " './my_model/tokenizer.model',\n",
              " './my_model/added_tokens.json',\n",
              " './my_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "trainer.save_model(\"./my_model\")\n",
        "train_tokenizer.save_pretrained(\"./my_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60029921-47ee-4682-b6a7-8e5107c3a9ae",
      "metadata": {
        "id": "60029921-47ee-4682-b6a7-8e5107c3a9ae"
      },
      "source": [
        "## Carregando o modelo do disco para teste\n",
        "- Nesse ponto pode ser interessante reiniciar a sess√£o do notebook para liberar mem√≥ria e carregar apenas o modelo treinado\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "md = Modelo('./my_model')\n",
        "md.print_pergunta_resposta(\"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\")"
      ],
      "metadata": {
        "id": "hbpPwdttQtjb"
      },
      "id": "hbpPwdttQtjb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Carregando do disco e testando o modelo base\n",
        "md = Modelo(model_name)\n",
        "md.print_pergunta_resposta(\"O que √© PUIL e qual o n√∫mero da lei que criou o PUIL?\")"
      ],
      "metadata": {
        "id": "QZPnCcg3eIzm"
      },
      "id": "QZPnCcg3eIzm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}