# -*- coding: utf-8 -*-
"""
Compara√ß√£o de extra√ß√µes de espelhos usando m√∫ltiplas m√©tricas de similaridade e configura√ß√£o via YAML.

Autor: Luiz An√≠sio
Refatorado: 25/01/2026
Descri√ß√£o:
-----------
Compara dados extra√≠dos por diferentes abordagens e modelos baseados em um arquivo de configura√ß√£o YAML.
Suporta m√∫ltiplas m√©tricas (BERTScore, ROUGE, Levenshtein) configur√°veis por campo.
Gera planilhas Excel, CSVs de estat√≠sticas e gr√°ficos comparativos.

Uso:
    python comparar_extracoes.py config_summa.yaml
"""

import os
import sys
import argparse
import yaml
import regex as re
import pandas as pd

# ============================================================================
# PROTE√á√ÉO E SETUP INICIAL
# ============================================================================
# Adiciona paths de utilit√°rios
sys.path.extend(['./utils', './src', '../../src'])

# Verifica√ß√£o para multiprocessing (BERTScore safe)
_IS_MAIN_PROCESS = __name__ == '__main__' or not hasattr(sys.modules.get('__mp_main__', None), '__file__')

def ler_configuracao(caminho_yaml):
    """L√™ e valida o arquivo de configura√ß√£o YAML."""
    if not os.path.exists(caminho_yaml):
        raise FileNotFoundError(f"Arquivo de configura√ß√£o n√£o encontrado: {caminho_yaml}")
    
    with open(caminho_yaml, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    print(f"üìñ Configura√ß√£o carregada de: {caminho_yaml}")
    return config

def _inicializar_ambiente():
    """Inicializa ambiente (carrega .env e configura√ß√µes globais)."""
    from util import UtilEnv
    UtilEnv.carregar_env('.env', pastas=['../', './'])
    
    # Recupera configura√ß√µes de ambiente ou defaults
    max_workers = UtilEnv.get_int('MAX_WORKERS_ANALISE', 10)
    device_bert = UtilEnv.get_str('BERTSCORE_DEVICE', 'auto')
    
    return max_workers, device_bert

# ============================================================================
# FUN√á√ïES DE AN√ÅLISE
# ============================================================================

def processar_analise_estatistica(dados_analise, pasta_saida, config):
    """
    Executa a an√°lise estat√≠stica (LLM-as-a-Judge) baseada nos pares definidos no YAML.
    """
    if not config.get('execucao', {}).get('analise_estatistica', False):
        return

    print("\nüìä Iniciando An√°lise Estat√≠stica (LLM-as-a-Judge)...")
    
    try:
        from util_analise_estatistica import AnaliseEstatistica
    except ImportError:
        print("‚ùå M√≥dulo util_analise_estatistica n√£o encontrado.")
        return

    lista_dados = []
    pk = dados_analise.config.nome_campo_id
    
    rotulo_base = config['modelo_base']['rotulo']
    familia_base = config['modelo_base'].get('familia', 'Base')
    
    # Itera sobre os modelos de compara√ß√£o definidos no YAML
    # Itera sobre os modelos de compara√ß√£o definidos no YAML (respeitando flag ativo)
    modelos_ativos = [m for m in config.get('modelos_comparacao', []) if m.get('ativo', True)]
    
    for modelo in modelos_ativos:
        rotulo_agente = modelo['rotulo']
        nome_familia = modelo.get('familia', rotulo_agente)
        
        print(f"   Processando par: {familia_base} ({rotulo_base}) vs {nome_familia} ({rotulo_agente})...")
        
        if rotulo_base not in dados_analise.rotulos or rotulo_agente not in dados_analise.rotulos:
            print(f"      ‚ö†Ô∏è  Saltando: R√≥tulos {rotulo_base} ou {rotulo_agente} n√£o encontrados nos dados.")
            continue

        for item in dados_analise.dados:
            id_peca = item.get(pk)
            if not id_peca: continue
            
            tokens = dados_analise.get_tokens(id_peca)
            evals = dados_analise.get_avaliacao(id_peca)
            
            v1 = evals.get(f'{rotulo_base}_F1')
            v2 = evals.get(f'{rotulo_agente}_F1')
            
            c1 = tokens.get(f'{rotulo_base}_total', 1) or 1
            c2 = tokens.get(f'{rotulo_agente}_total', 1) or 1
            
            if v1 is not None and v2 is not None:
                lista_dados.append({
                    'valor1': v1,
                    'valor2': v2,
                    'custo1': c1,
                    'custo2': c2,
                    'familia': nome_familia
                })
    
    if not lista_dados:
        print("‚ùå Nenhum dado compat√≠vel encontrado para an√°lise estat√≠stica.")
        return

    df_stat = pd.DataFrame(lista_dados)
    arquivo_saida = os.path.join(pasta_saida, 'relatorio_analise_estatistica.md')
    
    analise = AnaliseEstatistica(df_stat, config={
        'rotulo1': 'Base',
        'rotulo2': 'Agente',
        'arquivo_saida': arquivo_saida
    })
    analise.processar_analise()
    analise.salvar_relatorio()
    print(f"\n‚úÖ An√°lise Estat√≠stica conclu√≠da e salva em: {arquivo_saida}")

def configurar_metricas(config_yaml):
    """Converte a configura√ß√£o YAML para o formato esperado pelo JsonAnalise."""
    conf_comp = config_yaml.get('configuracao_comparacao', {})
    campos = conf_comp.get('campos', {})
    
    # Estrutura base
    config_final = {
        'nivel_campos': conf_comp.get('nivel_campos', 1),
        'padronizar_simbolos': conf_comp.get('padronizar_simbolos', True),
        'rouge_stemmer': conf_comp.get('rouge_stemmer', True),
        'campos_bertscore': campos.get('bertscore') or [],
        'campos_rouge': campos.get('rouge_l') or [],
        'campos_rouge2': campos.get('rouge_2') or [],
        'campos_rouge1': campos.get('rouge_1') or [],
        'campos_levenshtein': campos.get('levenshtein') or [],
        # M√©tricas SBERT (Sentence-BERT)
        'campos_sbert': campos.get('sbert') or [],
        'campos_sbert_pequeno': campos.get('sbert_pequeno') or [],
        'campos_sbert_medio': campos.get('sbert_medio') or [],
        'campos_sbert_grande': campos.get('sbert_grande') or []
    }
    
    # Ajuste para teste r√°pido (desativa BERTScore e SBERT grande)
    if config_yaml.get('execucao', {}).get('teste_rapido', False):
        print("‚ö†Ô∏è  Modo TESTE R√ÅPIDO: Desabilitando BERTScore/SBERT e movendo campos para ROUGE-L.")
        # Move campos BERTScore para ROUGE-L
        campos_removidos = config_final['campos_bertscore']
        config_final['campos_bertscore'] = []
        for c in campos_removidos:
            if c not in config_final['campos_rouge']:
                config_final['campos_rouge'].append(c)
        # Move campos SBERT para ROUGE-L
        for sbert_key in ['campos_sbert', 'campos_sbert_pequeno', 'campos_sbert_medio', 'campos_sbert_grande']:
            campos_sbert = config_final[sbert_key]
            config_final[sbert_key] = []
            for c in campos_sbert:
                if c not in config_final['campos_rouge']:
                    config_final['campos_rouge'].append(c)
                
    return config_final

def extrair_campos_unicos(config_metricas):
    """Extrai lista √∫nica de todos os campos envolvidos na compara√ß√£o, preservando ordem."""
    todos_campos = []
    seen = set()
    
    # Ordem de prioridade na visualiza√ß√£o/processamento (inclui SBERT)
    metricas_ordem = ['campos_bertscore', 'campos_rouge', 'campos_rouge2', 'campos_rouge1', 'campos_levenshtein',
                      'campos_sbert', 'campos_sbert_pequeno', 'campos_sbert_medio', 'campos_sbert_grande']
    
    for chave in metricas_ordem:
        campos = config_metricas.get(chave, [])
        for c in campos:
            if c not in seen:
                seen.add(c)
                todos_campos.append(c)
    
    # Remove campos especiais como (global) que n√£o existem no JSON
    return [c for c in todos_campos if not c.startswith('(')]

# ============================================================================
# MAIN
# ============================================================================

def resolver_caminho(caminho_relativo, base_dir):
    """Resolve caminhos relativos baseado no diret√≥rio do arquivo de configura√ß√£o."""
    if os.path.isabs(caminho_relativo):
        return caminho_relativo
    return os.path.normpath(os.path.join(base_dir, caminho_relativo))

def _gerar_grafico_erros(dados_analise, pasta_saida):
    """Gera gr√°fico de barras empilhadas com status dos documentos por modelo."""
    from util_graficos import UtilGraficos, Cores
    import pandas as pd
    
    rotulos_modelos = dados_analise.rotulos_modelos
    stats = {m: {'Sucesso': 0, 'Erro': 0, 'Inexistente': 0} for m in rotulos_modelos}
    
    for linha in dados_analise.dados:
        for modelo in rotulos_modelos:
            val = linha.get(modelo)
            # Verifica status baseado no valor
            if val is None:
                stats[modelo]['Inexistente'] += 1
            elif isinstance(val, dict) and 'erro' in val:
                erro_msg = str(val['erro'])
                if 'Inexistente' in erro_msg:
                     stats[modelo]['Inexistente'] += 1
                else:
                     stats[modelo]['Erro'] += 1
            else:
                stats[modelo]['Sucesso'] += 1
                
    df_stats = pd.DataFrame(stats).transpose()
    
    # Ordena colunas para mapear na paleta RdYlGn (Red -> Green)
    # 1. Inexistente (Red)
    # 2. Erro (Yellow/Orange)
    # 3. Sucesso (Green)
    cols_ordem = ['Inexistente', 'Erro', 'Sucesso']
    # Garante que as colunas existem no DF
    cols_existentes = [c for c in cols_ordem] # Sempre cria as 3 para consist√™ncia de cor?
    
    # Garante que todas as colunas existem
    for c in cols_ordem:
        if c not in df_stats.columns:
            df_stats[c] = 0
            
    df_stats = df_stats[cols_ordem]
    
    arquivo = os.path.join(pasta_saida, 'status_modelos.png')
    UtilGraficos.gerar_grafico_empilhado(
        df_stats, 
        titulo='Status das Extra√ß√µes por Modelo',
        ylabel='Quantidade de Documentos',
        xlabel='Modelo',
        arquivo_saida=arquivo,
        paleta_cores=Cores.RdYlGn
    )
    print(f"   ‚úì Gr√°fico de status gerado: {os.path.basename(arquivo)}")

# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Comparador de Extra√ß√µes JSON via YAML")
    parser.add_argument('config_file', help="Caminho do arquivo de configura√ß√£o YAML")
    args = parser.parse_args()

    # 1. Carregar configura√ß√£o
    caminho_yaml_abs = os.path.abspath(args.config_file)
    base_dir_yaml = os.path.dirname(caminho_yaml_abs)
    config = ler_configuracao(caminho_yaml_abs)
    
    # 2. Inicializar ambiente
    max_workers_env, _ = _inicializar_ambiente()
    max_workers = config['execucao'].get('max_workers', max_workers_env)
    
    # 3. Setup de Pastas e R√≥tulos (Resolvendo caminhos)
    pasta_saida_raw = config['saida']['pasta']
    pasta_saida = resolver_caminho(pasta_saida_raw, base_dir_yaml)
    
    if not os.path.exists(pasta_saida):
        os.makedirs(pasta_saida)
        
    modelo_base = config['modelo_base']
    modelos_comp_all = config['modelos_comparacao']
    
    # Filtra apenas modelos ativos (padr√£o: ativo=True)
    modelos_comp = [m for m in modelos_comp_all if m.get('ativo', True)]
    
    # Log de itens ignorados
    ignorados = [m['rotulo'] for m in modelos_comp_all if not m.get('ativo', True)]
    if ignorados:
        print(f"‚ö†Ô∏è  Modelos ignorados explicitamente (ativo=false): {', '.join(ignorados)}")

    # Prepara listas para CargaDadosComparacao
    origem_raw = modelo_base['pasta']
    origem = resolver_caminho(origem_raw, base_dir_yaml)
    
    rotulo_origem = modelo_base.get('rotulo', 'BASE')
    # Read campo_id from YAML config (defined ahead of time for use later)
    rotulo_id = config.get('configuracao_comparacao', {}).get('nome_campo_id', 'id')
    
    pastas_destinos = [resolver_caminho(m['pasta'], base_dir_yaml) for m in modelos_comp]
    rotulos_destinos = [m['rotulo'] for m in modelos_comp]
    
    # Valida√ß√µes b√°sicas
    if not os.path.isdir(origem):
        print(f"‚ùå Pasta base n√£o encontrada: {origem}")
        sys.exit(1)
    
    # 4. Configura√ß√£o das M√©tricas
    config_comparacao = configurar_metricas(config)
    campos_comparacao = extrair_campos_unicos(config_comparacao)
    
    print("\n‚öôÔ∏è  Configura√ß√£o de Compara√ß√£o:")
    print(f"   Origem (Base): {origem} [{rotulo_origem}]")
    print(f"   Destinos: {len(pastas_destinos)} pastas")
    print(f"   Campos: {campos_comparacao}")
    print(f"   Sa√≠da: {pasta_saida}")
    
    # 5. Carga de Dados
    # Imports tardios para evitar problemas de circularidade ou init desnecess√°rio
    from util_json_carga import CargaDadosComparacao
    from util_json import JsonAnaliseDataFrame
    
    # Regex e M√°scaras
    config_masks = config.get('configuracao_comparacao', {}).get('mascaras', {})
    
    def to_regex(val, is_extracao=False):
        if not val: return None
        if val.startswith('^'): return val
        # Se n√£o √© regex expl√≠cito, assume sufixo e cria regex capture group
        # Escape do sufixo para seguran√ßa
        return f"^(.+){re.escape(val)}$"

    mascara_extracao = config_masks.get('extracao', r'^(\d{12})\.\d+\.\d*\.json$')
    # Garante que extracao seja tratada como regex se n√£o especificado (fallback legacy)
    # ou se user passar sufixo, converte.
    mascara_extracao = to_regex(mascara_extracao, is_extracao=True)
    
    mascara_tokens = to_regex(config_masks.get('tokens', '.json'))
    mascara_avaliacao = to_regex(config_masks.get('avaliacao', '.avaliacao.json'))
    mascara_observabilidade = to_regex(config_masks.get('observabilidade', '.obs.json'))

    print(f"\nüîç M√°scaras definidas:")
    print(f"   Extra√ß√£o: {mascara_extracao}")
    print(f"   Tokens:   {mascara_tokens}")
    print(f"   Aval.:    {mascara_avaliacao}")
    print(f"   Obs.:     {mascara_observabilidade}")

    carga = CargaDadosComparacao(
        pasta_origem=origem,
        pastas_destinos=pastas_destinos,
        rotulo_id=rotulo_id,
        rotulo_origem=rotulo_origem,
        rotulos_destinos=rotulos_destinos,
        campos_comparacao=campos_comparacao,
        mascara_extracao=mascara_extracao,
        mascara_tokens=mascara_tokens,
        mascara_avaliacao=mascara_avaliacao,
        mascara_observabilidade=mascara_observabilidade,
        pasta_log_erros=pasta_saida,
        ignorar_erro_extracao=config['execucao'].get('ignorar_erro_extracao', False)
    )
    
    dados_analise = carga.carregar()
    print(dados_analise.resumo())
    
    if not dados_analise.dados:
        print("‚ùå Nenhum dado encontrado!. Verifique as pastas e os padr√µes de nome de arquivo.")
        sys.exit(1)

    # 6. Analisador e Processamento
    nome_arquivo_base = config['saida'].get('arquivo_base', 'comparacao_resultados')
    arquivo_excel = os.path.join(pasta_saida, f'{nome_arquivo_base}.xlsx')
    
    # Checa reuso
    regerar = config['saida'].get('regerar_planilha_base', True)
    analisador_instanciado = False
    
    # Define flags de execu√ß√£o
    flag_graficos = config['execucao'].get('gerar_graficos', False)
    flag_llm = config['execucao'].get('llm_as_a_judge', False)
    
    # L√≥gica Principal de Execu√ß√£o ou Reuso
    if os.path.isfile(arquivo_excel) and not regerar:
        print(f"\n‚ö†Ô∏è  Arquivo Excel j√° existe e 'regerar_planilha_base' √© FALSE.")
        print(f"   Pulando re-an√°lise completa. Usando arquivo existente: {arquivo_excel}")
        
        # Se for para apenas gerar gr√°ficos ou LLM judge em cima do existente
        if flag_graficos or flag_llm:
            # mas talvez n√£o precise processar tudo se tiver hooks espec√≠ficos. 
            # O c√≥digo original instanciava tudo. Vamos instanciar para garantir consist√™ncia.
            print("   Instanciando analisador para opera√ß√µes em arquivo existente...")
            pasta_jsons = os.path.join(pasta_saida, 'jsons')
            analisador = JsonAnaliseDataFrame(
                dados_analise,
                config=config_comparacao,
                pasta_analises=pasta_jsons,  # JSON files in jsons subfolder
                max_workers=max_workers,
                incluir_valores_analise=True,
                gerar_exemplos_md=False,
                gerar_relatorio=False
            )
            analisador_instanciado = True
    else:
        # EXECU√á√ÉO COMPLETA
        print(f"\nüöÄ Iniciando an√°lise completa...")
        pasta_jsons = os.path.join(pasta_saida, 'jsons')
        analisador = JsonAnaliseDataFrame(
            dados_analise, # Passa o objeto JsonAnaliseDados carregado
            config=config_comparacao,
            pasta_analises=pasta_jsons,  # JSON files in jsons subfolder
            pasta_markdown=pasta_saida,  # Markdown files in main folder (if supported)
            max_workers=max_workers,
            incluir_valores_analise=True,
            gerar_exemplos_md=True,
            max_exemplos_md_por_metrica=5,
            gerar_relatorio=True
        )
        analisador_instanciado = True
        
        # Relat√≥rio Markdown Overview
        if analisador.relatorio:
            analisador.relatorio.set_overview(
                titulo=f"Compara√ß√£o {rotulo_origem} vs Modelos",
                descricao="An√°lise comparativa configurada via YAML.",
                rotulos=analisador.rotulos,
                total_documentos=len(dados_analise.dados),
                campos_comparacao=campos_comparacao
            )
            analisador.relatorio.set_config(config_comparacao, campos_comparacao)
            
        # Gera o DataFrame e Exporta
        # Gera o DataFrame e Exporta
        print("üìä Exportando CSVs e Excel...")
        analisador.to_df() # Gera intenamente
        
        # Exporta CSV para a pasta raiz (usa caminho absoluto para sair da pasta_analises/jsons)
        arquivo_csv = os.path.join(pasta_saida, f'{nome_arquivo_base}.csv')
        analisador.exportar_csv(arquivo_csv)
        
        # Exporta Excel para a pasta raiz (usa caminho absoluto)
        analisador.exportar_excel(
            arquivo_excel, 
            incluir_estatisticas=True, 
            usar_formatacao_avancada=True,
            congelar_paineis=True,
            gerar_graficos=False # Gr√°ficos gerados no passo seguinte se solicitado
        )
        print(f"‚úÖ An√°lise Base salva em: {arquivo_excel}")

    # 7. P√≥s-Processamento (Gr√°ficos, LLM Judge, Stats)
    
    if flag_graficos and analisador_instanciado:
        print("\nüìà Gerando/Atualizando Gr√°ficos no Excel...")
        # Gera gr√°fico de status/erros
        _gerar_grafico_erros(dados_analise, pasta_saida)
        
        if os.path.isfile(arquivo_excel):
            analisador.gerar_graficos_de_excel(arquivo_excel, pasta_saida=pasta_saida)
    
    if flag_llm and analisador_instanciado:
        print("\n‚öñÔ∏è  Executando LLM-as-a-Judge (atualiza√ß√£o do Excel)...")
        if os.path.isfile(arquivo_excel):
            analisador.atualizar_avaliacao_llm_no_excel(arquivo_excel, gerar_graficos=True)

    if config['execucao'].get('analise_estatistica', False):
        processar_analise_estatistica(dados_analise, pasta_saida, config)

    # 8. Estat√≠sticas Finais no Console
    if analisador_instanciado:
        print("\nüìà Estat√≠sticas Globais (Resumo):")
        stats = analisador.estatisticas_globais()
        
        if not stats.empty:
            # Prioridade de m√©tricas para determinar melhor modelo (ordem de prefer√™ncia)
            # 1. Avalia√ß√£o LLM (se dispon√≠vel)
            # 2. BERTScore F1
            # 3. ROUGE-L F1
            # 4. ROUGE-2 F1
            # 5. ROUGE-1 F1
            metricas_prioridade = [
                r'\(global\)_llm_.*',       # LLM evaluation
                r'\(global\)_bertscore_F1', # BERTScore
                r'\(global\)_rouge_F1',     # ROUGE-L (rouge padr√£o = rouge-L)
                r'\(global\)_rougel_F1',    # ROUGE-L alternativo
                r'\(global\)_rouge2_F1',    # ROUGE-2
                r'\(global\)_rouge1_F1',    # ROUGE-1
            ]
            
            metrica_escolhida = None
            for padrao in metricas_prioridade:
                match = stats[stats['metrica'].str.contains(padrao, regex=True, na=False)]
                if not match.empty:
                    metrica_escolhida = match.iloc[0]['metrica']
                    break
            
            if metrica_escolhida:
                f1_topo = stats[stats['metrica'] == metrica_escolhida]
                if not f1_topo.empty:
                    idx_vencedor = f1_topo['mean'].idxmax()
                    vencedor = f1_topo.loc[idx_vencedor]
                    print(f"\nüèÜ Melhor Modelo em '{metrica_escolhida}':")
                    print(f"   {vencedor['modelo']} (M√©dia: {vencedor['mean']:.4f})")

    print("\n‚úÖ Processo finalizado com sucesso.")

if __name__ == '__main__':
    main()
