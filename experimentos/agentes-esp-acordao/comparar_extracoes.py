# -*- coding: utf-8 -*-
"""
Compara√ß√£o de extra√ß√µes de espelhos usando m√∫ltiplas m√©tricas de similaridade.

Autor: Luiz An√≠sio
Fonte: https://github.com/luizanisio/llms/tree/main/experimentos/agentes-esp-acordao
Data: 14/11/2025

Descri√ß√£o:
-----------
Compara espelhos extra√≠dos por diferentes abordagens (RAW, base, agentes) e modelos
(GPT-5, Gemma-3 12b/27b) usando BERTScore, ROUGE-L, ROUGE-2 e Levenshtein.
Seleciona m√©tricas apropriadas para cada tipo de campo conforme filosofia documentada.
"""

import os
import sys

sys.path.extend(['./utils','./src','../../src'])
import regex as re
from util import UtilEnv, Util
UtilEnv.carregar_env('.env', pastas=['../','./'])

from util_json import JsonAnaliseDataFrame
from util_json_carga import CargaDadosComparacao
from util_bertscore import configurar_bertscore_workers
''' 
  CONSTANTES E CONFIGURA√á√ïES DE VARI√ÅVEIS DE AMBIENTE
'''
max_workers_bert = UtilEnv.get_int('BERT_WORKERS', 10)
MAX_WORKERS_ANALISE = UtilEnv.get_int('MAX_WORKERS_ANALISE', 10)
PASTA_ENTRADA_RAIZ = os.getenv('PASTA_ENTRADA_RAIZ') or './saidas/'
configurar_bertscore_workers(max_workers = max_workers_bert)
'''
Compara com JsonAnalise os espelhos RAW, base e extra√ß√µes feitas pelos agentes.

FILOSOFIA DE SELE√á√ÉO DE M√âTRICAS:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
1. BERTScore ‚Üí Textos longos com nuances sem√¢nticas
2. ROUGE-L   ‚Üí Estruturas/sequ√™ncias ordenadas
3. ROUGE-2   ‚Üí Frases m√©dias, precis√£o de bigramas
4. ROUGE-1   ‚Üí Termos individuais, palavras-chave (padr√£o para (estrutura))
5. Levenshtein ‚Üí Textos curtos exatos (nomes, IDs, valores num√©ricos)

RAZ√ïES:   
‚ú® BENEF√çCIO: Cada campo √© analisado pela m√©trica mais adequada ao seu tipo,
   gerando m√∫ltiplas perspectivas onde necess√°rio (ex: teseJuridica tem tanto
   sem√¢ntica profunda quanto precis√£o de fraseamento).
'''
ORIGEM, DESTINOS, D_ROTULOS, CAMPOS_COMPARACAO, PASTA_SAIDA_COMPARACAO, ROTULO_ID, ROTULO_ORIGEM = None, None, None, None, None, None, None
TESTE = False
def base_raw():
    global ORIGEM, DESTINOS, D_ROTULOS, CAMPOS_COMPARACAO, PASTA_SAIDA_COMPARACAO, ROTULO_ID, ROTULO_ORIGEM
    ORIGEM = 'espelhos_raw/'
    DESTINOS = ['espelhos_base_gpt5/', 'espelhos_agentes_gpt5/', 'espelhos_base_gemma3_12b/', 'espelhos_agentes_gemma3_12b/', 'espelhos_base_gemma3_27b/', 'espelhos_agentes_gemma3_27b/']
    D_ROTULOS = ['base_gpt5','agentes_gpt5','base_gemma3(12)','agentes_gemma3(12)','base_gemma3(27)','agentes_gemma3(27)']
    ROTULO_ID = 'id'
    ROTULO_ORIGEM = 'RAW'
    CAMPOS_COMPARACAO = ['jurisprudenciaCitada','notas','informacoesComplementares', 'termosAuxiliares', 'teseJuridica', 'tema', 'referenciasLegislativas']
    PASTA_SAIDA_COMPARACAO = 'analises_comparacao_raw/'
def base_gpt5():
    global ORIGEM, DESTINOS, D_ROTULOS, CAMPOS_COMPARACAO, PASTA_SAIDA_COMPARACAO, ROTULO_ID, ROTULO_ORIGEM
    ORIGEM = 'espelhos_base_gpt5/'
    DESTINOS = ['espelhos_agentes_gpt5/', 'espelhos_base_gemma3_12b/', 'espelhos_agentes_gemma3_12b/', 'espelhos_base_gemma3_27b/', 'espelhos_agentes_gemma3_27b/']
    D_ROTULOS = ['agentes_gpt5','base_gemma3(12)','agentes_gemma3(12)','base_gemma3(27)','agentes_gemma3(27)']
    ROTULO_ID = 'id'
    ROTULO_ORIGEM = 'base_gpt5'
    CAMPOS_COMPARACAO = ['jurisprudenciaCitada','notas','informacoesComplementares', 'termosAuxiliares', 'teseJuridica', 'tema', 'referenciasLegislativas']
    PASTA_SAIDA_COMPARACAO = 'analises_comparacao_base_gpt5/'
def base_gpt5_p():
    base_gpt5()
    global ORIGEM, PASTA_SAIDA_COMPARACAO, TESTE, DESTINOS, D_ROTULOS
    #DESTINOS = ['espelhos_agentes_gpt5/', 'espelhos_base_gemma3_12b/', 'espelhos_base_gemma3_27b/']
    #D_ROTULOS = ['agentes_gpt5','base_gemma3(12)','base_gemma3(27)']
    ORIGEM = 'espelhos_base_p/'
    PASTA_SAIDA_COMPARACAO = 'analises_comparacao_teste/'
    TESTE = True # n√£o usa bertscore para teste r√°pido
def base_gpt5_ag():
    base_gpt5_p()
    global ORIGEM
    ORIGEM = 'espelhos_agentes_p/'
def base_gpt5_g():
    base_gpt5_p()
    global ORIGEM, TESTE
    ORIGEM = 'espelhos_base_gpt5/'
    TESTE = True # n√£o usa bertscore para teste r√°pido
base_gpt5()
assert len(DESTINOS) == len(D_ROTULOS), 'N√∫mero de destinos e r√≥tulos deve ser igual!'
ORIGEM = os.path.join(PASTA_ENTRADA_RAIZ, ORIGEM)
DESTINOS = [os.path.join(PASTA_ENTRADA_RAIZ, d) for d in DESTINOS]
PASTA_SAIDA_COMPARACAO=os.path.join(PASTA_ENTRADA_RAIZ, PASTA_SAIDA_COMPARACAO)
print('Pasta ra√≠z:', PASTA_ENTRADA_RAIZ)
print('Origem:', ORIGEM)
print('Destinos:', DESTINOS)
print('Sa√≠da:', PASTA_SAIDA_COMPARACAO)
assert os.path.isdir(ORIGEM), f'Pasta de origem "{ORIGEM}" n√£o existe!'
for d in DESTINOS:
    assert os.path.isdir(d), f'Pasta de destino "{d}" n√£o existe: {d}'

# Configura√ß√£o otimizada para nova estrutura JsonAnalise (sem metrica_global)
CONFIG_COMPARACAO = {
    # N√≠vel de campos (1 = apenas raiz, 2 = raiz + 1 n√≠vel aninhado)
    'nivel_campos': 1,
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CAMPOS COM M√öLTIPLAS M√âTRICAS (an√°lise multidimensional)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # BERTScore: similaridade sem√¢ntica profunda (textos longos)
    'campos_bertscore': [
        '(global)',                  # Vis√£o geral do documento
        'teseJuridica',              # Teses jur√≠dicas complexas + ROUGE-L (sem√¢ntica + precis√£o)
        'notas',                     # Textos descritivos (admite parafraseamento)
        'termosAuxiliares',          # Lista de termos t√©cnicos + ROUGE-2 (sem√¢ntica + bigramas)
        'informacoesComplementares'  # Informa√ß√µes adicionais (texto livre)
    ],
    
    # ROUGE-L: sequ√™ncias estruturadas (ordem importa)
    'campos_rouge': [
        '(global)',                   # Vis√£o geral do documento
        'jurisprudenciaCitada',       # Cita√ß√µes estruturadas + ROUGE-2 (estrutura + bigramas)
        'informacoesComplementares',  # Informa√ß√µes adicionais (texto livre)
        'referenciasLegislativas',    # Refer√™ncias legais (estrutura Lei/Art/¬ß)
        'notas',                     # Textos descritivos (admite parafraseamento)
        'teseJuridica',               # + BERTScore (valida fraseamento legal exato)
    ],
    
    # ROUGE-2: precision de bigramas (fraseamento t√©cnico e termos exatos)
    'campos_rouge2': [
        'termosAuxiliares',          # + BERTScore (bigramas t√©cnicos)
        'tema',                      # Temas como frases curtas
        'jurisprudenciaCitada',       # Cita√ß√µes estruturadas + ROUGE-2 (estrutura + bigramas)
        # (global) ser√° adicionado automaticamente aqui se n√£o estiver em outra m√©trica
    ],
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # OBSERVA√á√ÉO: (global) e (estrutura) recebem m√©tricas padr√£o autom√°ticas:
    # - (global) ‚Üí campos_rouge2 (se n√£o especificado em outra lista)
    # - (estrutura) ‚Üí campos_rouge1 (se n√£o especificado em outra lista)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Configura√ß√µes de processamento
    'padronizar_simbolos': True,    # Normaliza aspas, espa√ßos, case
    'rouge_stemmer': True           # Usa stemmer no ROUGE para varia√ß√µes morfol√≥gicas
}

if TESTE:
    # Configura√ß√£o r√°pida para testes (sem BERTScore)
    campos_bertscore = CONFIG_COMPARACAO.get('campos_bertscore', [])
    CONFIG_COMPARACAO['campos_bertscore'] = []
    CONFIG_COMPARACAO['campos_levenshtein'] = ['termosAuxiliares', 'referenciasLegislativas']  # usa Levenshtein para termosAuxiliares nos testes
    campos_bertscore = [_ for _ in campos_bertscore if _ not in CONFIG_COMPARACAO['campos_rouge2']]
    _linha = '‚ö†Ô∏è  ' * 20
    print(f'\n{_linha}\nModo TESTE ativado: BERTScore desabilitado:\n - campos movidos para Rouge 2: {campos_bertscore}\n{_linha}\n')
    CONFIG_COMPARACAO['campos_rouge2'] += campos_bertscore
    Util.pausa(3)


    
if __name__ == '__main__':
    ''' realiza a compara√ß√£o das extra√ß√µes dos espelhos na pasta ORIGEM com as extra√ß√µes nas pastas DESTINOS
        todas as pastas devem conter arquivos json nomeados com o id_peca.json, outros arquivos s√£o ignorados
        caso o arquivo n√£o exista em uma das pastas, √© registrado como "Inexistente"
        caso exista uma chave "erro" no json, √© registrado como "Erro na extra√ß√£o"
        caso o campo origem seja nulo ou vazio e no destino tamb√©m, os campos podem ser removidos na compara√ß√£o
        o resultado √© salvo conforme exemplo no arquivo "exemplo_dataframe.py"
    '''
    
    print("=" * 80)
    print("üîç COMPARA√á√ÉO DE EXTRA√á√ïES - Espelhos RAW vs Base vs Agentes")
    print("=" * 80)
    
    # tipo de arquivo \d{12}.\d+.\d*.json (padr√£o)
    RE_ARQUIVOS_JSON_PADRAO = re.compile(r'^(\d{12})\.\d+\.\d*\.json$')
    
        # Instancia a classe de carga de dados
    carga = CargaDadosComparacao(
        pasta_origem=ORIGEM,
        pastas_destinos=DESTINOS,
        rotulo_id=ROTULO_ID,
        rotulo_origem=ROTULO_ORIGEM,
        rotulos_destinos=D_ROTULOS,
        campos_comparacao=CAMPOS_COMPARACAO,
        regex_arquivos=RE_ARQUIVOS_JSON_PADRAO
    )
    
    # Carrega os dados - agora retorna JsonAnaliseDados completo
    dados_analise = carga.carregar()
    
    # Exibe resumo dos dados
    print(dados_analise.resumo())

    if not dados_analise.dados:
        print("\n‚ùå Nenhum dado encontrado para compara√ß√£o!")
        sys.exit(1)
    
    
    print(f"\n‚öôÔ∏è  Configura√ß√£o de compara√ß√£o:")
    print(f"   Campos analisados: {len(CAMPOS_COMPARACAO)}")
    print(f"   N√≠vel de campos: {CONFIG_COMPARACAO.get('nivel_campos')}")
    print(f"   Campos BERTScore: {len(CONFIG_COMPARACAO.get('campos_bertscore', []))} ‚Üí {CONFIG_COMPARACAO.get('campos_bertscore', [])}")
    print(f"   Campos ROUGE-L: {len(CONFIG_COMPARACAO.get('campos_rouge', []))} ‚Üí {CONFIG_COMPARACAO.get('campos_rouge', [])}")
    print(f"   Campos ROUGE-1: {len(CONFIG_COMPARACAO.get('campos_rouge1', []))} ‚Üí {CONFIG_COMPARACAO.get('campos_rouge1', [])}")
    print(f"   Campos ROUGE-2: {len(CONFIG_COMPARACAO.get('campos_rouge2', []))} ‚Üí {CONFIG_COMPARACAO.get('campos_rouge2', [])} (+ (global) autom√°tico)")
    print(f"   üìå Nota: (estrutura) ser√° adicionado automaticamente em ROUGE-1")

    # Cria analisador
    print(f"\nüöÄ Iniciando an√°lise com JsonAnaliseDataFrame...")
    analisador = JsonAnaliseDataFrame(
        dados_analise,  # Nova interface: passa JsonAnaliseDados
        config=CONFIG_COMPARACAO,
        pasta_analises=PASTA_SAIDA_COMPARACAO,
        max_workers=MAX_WORKERS_ANALISE,
        incluir_valores_analise=True,  # incluir valores nos JSONs de an√°lise
        gerar_exemplos_md=True,  # Gera arquivo Markdown com exemplos
        max_exemplos_md_por_metrica=5  # M√°ximo de 5 exemplos por m√©trica
    )

    # Define nome base dos arquivos (usado pelos m√©todos de exporta√ß√£o)
    nome_arquivo_base = 'comparacao_extracoes'
    arquivo_excel = os.path.join(PASTA_SAIDA_COMPARACAO, f'{nome_arquivo_base}.xlsx')

    SO_GRAFICOS = False  # Define como True para gerar apenas gr√°ficos de Excel existente
    if SO_GRAFICOS:
       # Apenas atualiza os gr√°ficos do excel j√° existente 
        if os.path.isfile(arquivo_excel):
            print(f"\n‚ö†Ô∏è  Aviso: O arquivo Excel de compara√ß√£o j√° existe: {arquivo_excel}\nGerando gr√°ficos...")
            analisador.gerar_graficos_de_excel(arquivo_excel, pasta_saida=PASTA_SAIDA_COMPARACAO)
            exit(0)
    
    SO_LLM_AS_A_JUDGE = True  # Define como True para usar LLM as a Judge
    if SO_LLM_AS_A_JUDGE:
        # Apenas atualiza as an√°lises de LLM as a Judge do Excel existente
        if os.path.isfile(arquivo_excel):
            print(f"\n‚ö†Ô∏è  Aviso: O arquivo Excel de compara√ß√£o j√° existe: {arquivo_excel}\nAtualizando com an√°lises de LLM as a Judge...")
            # Atualiza apenas a aba de avalia√ß√£o LLM
            analisador.atualizar_avaliacao_llm_no_excel(arquivo_excel, gerar_graficos=True)
            print(f"\n‚úÖ Aba 'Avalia√ß√£o LLM' atualizada com sucesso!")
            print(f"üìÅ Arquivo: {arquivo_excel}")
            exit(0)
        else:
            print(f"\n‚ùå Erro: Arquivo Excel n√£o encontrado: {arquivo_excel}")
            print(f"   Execute primeiro sem SO_LLM_AS_A_JUDGE=True para gerar o arquivo base.")
            exit(1)
    
    # Gera DataFrame
    print("üìä Gerando DataFrame...")
    df = analisador.to_df()
    
    print(f"\n‚úÖ An√°lise conclu√≠da!")
    print(f"   Documentos processados: {len(df)}")
    print(f"   Colunas geradas: {len(df.columns)}")
    
    # Mostra estat√≠sticas globais
    print("\nüìà Estat√≠sticas Globais:")
    stats = analisador.estatisticas_globais()
    
    # NOVA ESTRUTURA: M√∫ltiplas t√©cnicas para (global)
    # Exemplos: (global)_rouge2_F1, (global)_rouge1_F1, etc.
    print("\n   üìä F1-Score por T√©cnica (campo global):")
    
    # Agrupa por t√©cnica
    tecnicas_disponiveis = stats['tecnica'].unique()
    for tecnica in sorted(tecnicas_disponiveis):
        # Busca F1 global dessa t√©cnica
        metrica_busca = f'(global)_{tecnica.lower().replace("-", "")}_F1'
        f1_tecnica = stats[stats['metrica'] == metrica_busca]
        
        if len(f1_tecnica) > 0:
            print(f"\n   {tecnica}:")
            for _, row in f1_tecnica.iterrows():
                print(f"      {row['modelo']:15s}: Mean={row['mean']:.4f}, Median={row['median']:.4f}, Std={row['std']:.4f}")
    
    # Mostra melhor m√©trica global (tenta ROUGE-2 primeiro, depois qualquer outra)
    f1_global = stats[stats['metrica'] == '(global)_rouge2_F1']
    if len(f1_global) == 0:
        # Fallback: tenta qualquer (global)_*_F1
        f1_global = stats[stats['metrica'].str.contains(r'\(global\)_.*_F1', regex=True)]
    
    # Mostra compara√ß√£o de modelos (usa m√©trica dispon√≠vel)
    if len(f1_global) > 0:
        metrica_comparacao = f1_global.iloc[0]['metrica']
        print(f"\nüîç Compara√ß√£o por documento (m√©trica: {metrica_comparacao}):")
        try:
            comp_f1 = analisador.comparar_modelos(metrica_comparacao)
            print(comp_f1.head(10).to_string(index=False))
        except ValueError as e:
            print(f"\n   ‚ö†Ô∏è  Erro ao comparar modelos: {e}")
    else:
        print("\n   ‚ö†Ô∏è  Nenhuma m√©trica global F1 dispon√≠vel para compara√ß√£o")
    
    # Exporta resultados
    print("\nüíæ Exportando resultados...")
    
    # CSV (m√©todo retorna o caminho do arquivo gerado)
    arquivo_csv = analisador.exportar_csv(nome_arquivo_base)
    arquivo_estatisticas = arquivo_csv.replace('.csv', '.estatisticas.csv')
    print(f"   ‚úì CSV: {arquivo_csv}")
    print(f"   ‚úì Estat√≠sticas CSV: {arquivo_estatisticas}")
    
    # Excel com formata√ß√£o avan√ßada (mapas de calor)
    print("\n   Gerando Excel formatado com mapas de calor...")
    arquivo_excel = analisador.exportar_excel(
        nome_arquivo_base,  # M√©todo adiciona .xlsx automaticamente
        incluir_estatisticas=True,
        usar_formatacao_avancada=True,  # Usa UtilPandasExcel com mapas de calor
        congelar_paineis=True,
        gerar_graficos=True  # Gr√°ficos gerados separadamente
    )
    print(f"   ‚úì Excel formatado: {arquivo_excel}")
    print(f"      ‚Ä¢ Aba 'Resultados': m√©tricas por documento com mapa de calor")
    print(f"      ‚Ä¢ Aba 'Estat√≠sticas': agrega√ß√µes globais")
    print(f"      ‚Ä¢ Aba 'Compara√ß√£o_F1': compara√ß√£o de modelos")
    
    # Resumo final
    print("\nüìä Resumo Final:")
    print(f"   Total de campos comparados: {len(CAMPOS_COMPARACAO)}")
    print(f"   Campos: {', '.join(CAMPOS_COMPARACAO[:3])}...")
    
    # Melhor modelo por F1 (usa qualquer m√©trica global dispon√≠vel)
    if len(f1_global) > 0:
        modelo_vencedor = f1_global.loc[f1_global['mean'].idxmax(), 'modelo']
        f1_vencedor = f1_global.loc[f1_global['mean'].idxmax(), 'mean']
        metrica_vencedor = f1_global.iloc[0]['metrica']
        print(f"\nüèÜ Melhor modelo ({metrica_vencedor}): {modelo_vencedor} (Mean={f1_vencedor:.4f})")
        
        # Mostra tamb√©m outras t√©cnicas do vencedor
        print(f"\n   Todas as m√©tricas do modelo vencedor ({modelo_vencedor}):")
        stats_vencedor = stats[(stats['modelo'] == modelo_vencedor) & (stats['metrica'].str.contains(r'\(global\)_.*_F1', regex=True))]
        for _, row in stats_vencedor.iterrows():
            tecnica = row['tecnica']
            print(f"      {tecnica:12s} F1: Mean={row['mean']:.4f}, Median={row['median']:.4f}, Std={row['std']:.4f}")
    else:
        print("\n   ‚ö†Ô∏è  Estat√≠sticas n√£o dispon√≠veis para exibir vencedor")
    
    print("\n" + "=" * 80)
    print("‚úÖ Compara√ß√£o conclu√≠da com sucesso!")
    print(f"üìÅ Resultados salvos em: {PASTA_SAIDA_COMPARACAO}")
    print("=" * 80)


