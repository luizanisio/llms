## Orquestra√ß√£o de Agentes-LLM para extra√ß√£o de metadados na gera√ß√£o do Espelho do Ac√≥rd√£o

Este projeto implementa e compara abordagens para a extra√ß√£o de metadados estruturados (Espelhos de Ac√≥rd√£os) a partir de textos jur√≠dicos do STJ. O experimento contrasta uma abordagem tradicional de prompt √∫nico ("Base") com uma arquitetura de m√∫ltiplos agentes especializados ("Orquestra√ß√£o"), avaliando os resultados atrav√©s de m√©tricas cl√°ssicas e LLM-as-a-Judge.

**üìä [Ver todos os diagramas de fluxo e arquitetura](README_MERMAID.md)**

### Como extrair os dados da origem "Dados abertos" do STJ com o "ckan"

O script principal para esta etapa √© `ckan_extrair_espelhos.py`.

- **Configura√ß√µes essenciais**:
  - A lista `DATASET_IDs` define os conjuntos de dados (turmas e se√ß√µes) a serem baixados do portal de Dados Abertos do STJ.
  - O script consolida os dados baixados e os textos dos ac√≥rd√£os em um arquivo Parquet (`espelhos_acordaos_consolidado_textos.parquet`) e CSVs auxiliares.

- **Sa√≠da esperada**:
  - Arquivos JSON originais na pasta `downloads_esp_stj`.
  - Arquivos consolidados para uso nos scripts de gera√ß√£o.
  - O script `converter_dados_abertos.py` deve ser executado posteriormente para converter os dados originais para o formato JSON padronizado (`espelhos_raw`), servindo como *Ground Truth* para as compara√ß√µes.

### Gera√ß√£o do espelho com prompt base

Esta abordagem utiliza um √∫nico prompt complexo para extrair todos os campos de uma vez.

- **Como organizar os dados**:
  - Certifique-se de que o arquivo `espelhos_acordaos_consolidado_textos.parquet` foi gerado na etapa anterior.
  - Configure as vari√°veis de ambiente (chaves de API) no arquivo `.env`.

- **Como rodar a gera√ß√£o**:
  - Execute o script `gerar_espelho_sjr_base.py`.
  - Ele utiliza o prompt definido em `prompt_espelho_base.py` (`PROMPT_BASE_SJR_S3_JSON`).
  - Os resultados s√£o salvos na pasta `saidas/espelhos_base/`.

### Gera√ß√£o do espelho com a orquestra√ß√£o de agentes

Esta abordagem divide a tarefa entre v√°rios agentes especializados coordenados por um orquestrador.

- **Como organizar os dados**:
  - Utiliza a mesma base de dados consolidada em Parquet.

- **Como rodar a orquestra√ß√£o**:
  - Execute o script `agentes_gerar_espelhos.py`.
  - Este script instancia a classe `AgenteOrquestradorEspelho` (de `agentes_orquestrador.py`).
  - Os prompts espec√≠ficos para cada agente est√£o em `prompt_espelho_agentes.py`.
  - Os resultados s√£o salvos em pastas espec√≠ficas por modelo (ex: `saidas/espelhos_agentes_gpt5/`).
  - **üìä [Ver diagrama detalhado do fluxo de orquestra√ß√£o](README_MERMAID.md#2-fluxo-de-orquestra√ß√£o-completo-sistema-de-agentes)**

**Pipeline de Execu√ß√£o:**

1. **ETAPA 1**: `AgenteCampos` - Identifica quais campos existem no ac√≥rd√£o
2. **ETAPA 1.5**: Revis√£o do `AgenteCampos` - Se n√£o identificou campos, solicita revis√£o com instru√ß√£o espec√≠fica para conferir atentamente
3. **ETAPA 2**: `AgenteTeses` - Extrai as teses jur√≠dicas (depend√™ncia prim√°ria)
4. **ETAPA 2.5**: `AgenteJurisprudenciasCitadas` - Extrai precedentes baseados nas teses extra√≠das
5. **ETAPA 3**: Execu√ß√£o Paralela - `AgenteNotas`, `AgenteInformacoesComplementares`, `AgenteTermosAuxiliares`, `AgenteTema` e `AgenteReferenciasLegislativas` rodam simultaneamente
6. **ETAPA 4**: `AgenteValidacaoFinal` - Consolida e valida todas as extra√ß√µes
7. **ETAPA 5**: Loop de Revis√£o - Processa at√© 2 ciclos de revis√µes conforme necess√°rio, reexecutando agentes com erros ou que precisam de ajustes
8. **Consolida√ß√£o Final**: Monta o espelho final com todos os campos extra√≠dos e metadados
9. **Verifica√ß√£o de Erros**: Apenas grava arquivos se n√£o houver erros remanescentes, permitindo novas tentativas em caso de falha

### Avalia√ß√£o LLM-as-a-judge

Utiliza um modelo avan√ßado (GPT-5) para avaliar a qualidade sem√¢ntica das extra√ß√µes.

- **Como organizar os arquivos e rodar a avalia√ß√£o**:
  - Execute `avaliacao_llm_as_a_judge.py`.
  - O script percorre as pastas de sa√≠da (Base e Agentes) e compara cada extra√ß√£o com o texto original do ac√≥rd√£o.
  - Calcula m√©tricas de **Precision**, **Recall** e **F1-Score** baseadas na interpreta√ß√£o do LLM Juiz.
  - Gera arquivos `.avaliacao.json` junto aos arquivos extra√≠dos.
  - **üìä [Ver diagrama do fluxo de avalia√ß√£o](README_MERMAID.md#4-avalia√ß√£o-llm-as-a-judge)**

### Gera√ß√£o de planilha de compara√ß√µes

Realiza uma compara√ß√£o t√©cnica entre as extra√ß√µes geradas e o *Ground Truth* (Dados Abertos).

- **Como organizar os arquivos**:
  - As pastas de sa√≠da das gera√ß√µes (`espelhos_base_*`, `espelhos_agentes_*`) e a pasta de refer√™ncia (`espelhos_raw`) devem estar populadas.

- **Como rodar a avalia√ß√£o**:
  - Execute `comparar_extracoes.py`.
  - O script utiliza a classe `JsonAnaliseDataFrame` para aplicar m√©tricas espec√≠ficas para cada tipo de campo:
    - **BERTScore**: Para campos textuais longos e sem√¢nticos (ex: Teses).
    - **ROUGE-L/2**: Para sequ√™ncias e frases (ex: Jurisprud√™ncia).
    - **Levenshtein**: Para campos exatos.

- **Dados que a planilha consolida**:
  - Gera relat√≥rios comparativos que permitem visualizar a performance de cada modelo e abordagem (Base vs. Agentes) em rela√ß√£o aos dados oficiais.
  - **üìä [Ver diagramas de m√©tricas e compara√ß√£o](README_MERMAID.md#5-compara√ß√£o-de-extra√ß√µes-m√©tricas-de-similaridade)**