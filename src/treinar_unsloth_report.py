import os
import json
from typing import Dict, Any, Optional
from treinar_unsloth_util import YamlTreinamento

# ---------------------------------------------------------------------------
# Classe GeradorRelatorio
# ---------------------------------------------------------------------------

class GeradorRelatorio:
    """
    Gera relat√≥rios em Markdown sobre o treinamento, salvando na pasta 
    'treinamento' dentro do diret√≥rio de sa√≠da do modelo.
    """
    
    
    def __init__(self, yaml_config: YamlTreinamento):
        self.yaml_config = yaml_config
        self.output_dir = yaml_config.modelo.saida
        self.report_dir = os.path.join(self.output_dir, "treinamento")
        self.report_file = os.path.join(self.report_dir, "relatorio_treinamento.md")
        

        """
        Gera e salva o relat√≥rio.
        
        Args:
            dataset_stats: Estat√≠sticas do dataset (contagem, tokens, etc)
            train_stats: M√©tricas finais do treinamento (loss, tempo, etc)
            hardware_info: Informa√ß√µes da m√°quina (CPUs, Mem√≥ria, GPU)
            print_only: Se True, apenas imprime no console e n√£o salva arquivo.
        """
        # ... implementation ...


    def collect_token_stats(self, datasets: Dict[str, Any]) -> Dict[str, Any]:
        """
        Coleta estat√≠sticas de tokens para os datasets fornecidos usando UtilTikToken.
        
        Args:
            datasets: Dict com keys 'train', 'eval', 'test' contendo os datasets.
            
        Returns:
            Dict com estat√≠sticas de tokens para cada split.
        """
        from util_tiktoken import UtilTikToken
        
        try:
            tokenizer = UtilTikToken()
        except ImportError:
            print("‚ö†Ô∏è TikToken n√£o dispon√≠vel, estat√≠sticas de token ser√£o ignoradas.")
            return {}
            
        stats = {}
        
        # Nome do modelo para o tiktoken (usa o base model name)
        model_name = self.yaml_config.modelo.base
        
        # Carrega template se existir para estat√≠sticas diferenciadas
        template_tokens_base = 0
        has_template = False
        try:
            cfg_entrada = self.yaml_config.pastas.entrada
            if cfg_entrada.prompt_template and os.path.isfile(cfg_entrada.prompt_template):
                with open(cfg_entrada.prompt_template, 'r', encoding='utf-8') as f:
                    template_content = f.read()
                
                # Remove a tag para contar os tokens "fixos" do template
                # Isso √© uma aproxima√ß√£o, pois a tokeniza√ß√£o pode mudar nas fronteiras,
                # mas serve bem para estat√≠stica.
                tag = cfg_entrada.tag_texto or '<<--TEXTO-->>'
                template_base = template_content.replace(tag, '')
                template_tokens_base = tokenizer.contar_tokens(template_base, modelo=model_name)['qtd_tokens']
                has_template = True
                print(f"   üìÑ Template base carregado: {template_tokens_base} tokens (aprox)")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao processar template para estat√≠sticas: {e}")

        # Mapeamento de nomes de split para nomes de exibi√ß√£o
        splits = {
            'train': 'Treino',
            'eval': 'Valida√ß√£o',
            'test': 'Teste'
        }
        
        print(f"\nüìä Calculando estat√≠sticas de tokens (usando tiktoken)...")
        
        for split_key, display_name in splits.items():
            if split_key not in datasets or datasets[split_key] is None:
                continue
                
            dataset = datasets[split_key]
            if len(dataset) == 0:
                continue
                
            print(f"   Processando {display_name} ({len(dataset)} registros)...")
            
            total_tokens = 0
            total_unique_tokens = 0
            tokens_list = []
            
            # Listas para estat√≠sticas do CONTE√öDO (texto injetado)
            doc_tokens_list = []
            
            # Itera sobre os exemplos para contar tokens
            for item in dataset:
                texto_full = ""
                user_content = ""
                
                if 'messages' in item:
                    # Formato chat: messages list maps to string for total stats
                    # Tenta identificar o user prompt para estat√≠stica do documento
                    msgs = item['messages']
                    texto_full = json.dumps(msgs, ensure_ascii=False)
                    
                    # Assume primeira mensagem ou busca role='user'
                    for m in msgs:
                        if m.get('role') == 'user':
                            user_content = m.get('content', '')
                            break
                            
                elif 'prompt' in item and 'completion' in item:
                    # Formato legacy
                    user_content = item['prompt']
                    texto_full = f"{item['prompt']}\n{item['completion']}"
                elif 'text' in item:
                    texto_full = item['text']
                    # Dif√≠cil extrair o prompt base apenas do text formatado sem parser complexo
                else:
                    texto_full = str(item)
                    
                # 1. Estat√≠stica Geral (Exemplo completo)
                count = tokenizer.contar_tokens(texto_full, modelo=model_name)
                t_count = count['qtd_tokens']
                t_unique = count['qtd_tokens_unicos']
                
                tokens_list.append(t_count)
                total_tokens += t_count
                total_unique_tokens += t_unique
                
                # 2. Estat√≠stica do Texto Injetado (Doc Tokens)
                if has_template and user_content:
                    # Conta tokens do prompt do usu√°rio
                    user_count = tokenizer.contar_tokens(user_content, modelo=model_name)['qtd_tokens']
                    # Subtrai o template base
                    doc_tokens = max(0, user_count - template_tokens_base)
                    doc_tokens_list.append(doc_tokens)
                
            if tokens_list:
                stats[split_key] = {
                    'count': len(dataset),
                    'total_tokens': total_tokens,
                    'avg_tokens': round(total_tokens / len(dataset), 1),
                    'total_unique_tokens': total_unique_tokens,
                    'min_tokens': min(tokens_list),
                    'max_tokens': max(tokens_list)
                }
                
                # Adiciona estat√≠sticas do texto injetado se dispon√≠vel
                if doc_tokens_list:
                    stats[split_key]['doc_stats'] = {
                         'template_tokens': template_tokens_base,
                         'avg_doc_tokens': round(sum(doc_tokens_list) / len(doc_tokens_list), 1),
                         'min_doc_tokens': min(doc_tokens_list),
                         'max_doc_tokens': max(doc_tokens_list)
                    }
                
        return stats
    
    def gerar_relatorio(self, 
                        dataset_stats: Dict[str, Any] = None, 
                        train_stats: Dict[str, Any] = None,
                        hardware_info: Dict[str, Any] = None,
                        print_only: bool = False) -> str:
        """
        Gera e salva o relat√≥rio.
        
        Args:
            dataset_stats: Estat√≠sticas do dataset (contagem, tokens, etc)
            train_stats: M√©tricas finais do treinamento (loss, tempo, etc)
            hardware_info: Informa√ß√µes da m√°quina (CPUs, Mem√≥ria, GPU)
            print_only: Se True, apenas imprime no console e n√£o salva arquivo.
        """
        from datetime import datetime
        
        conteudo = []
        conteudo.append(f"# Relat√≥rio de Treinamento LLM")
        conteudo.append(f"**Data:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        conteudo.append(f"**Modelo Base:** `{self.yaml_config.modelo.base}`")
        conteudo.append(f"**Diret√≥rio de Sa√≠da:** `{self.output_dir}`")
        
        # 1. Configura√ß√£o
        conteudo.append("\n## 1. Configura√ß√£o Utilizada")
        conteudo.append("```yaml")
        conteudo.append(self.yaml_config.info())
        conteudo.append("```")
        
        # 2. Hardware
        if hardware_info:
            conteudo.append("\n## 2. Hardware")
            
            # Se√ß√£o CPU
            conteudo.append("\n### CPU")
            conteudo.append(f"- **CPUs F√≠sicas:** {hardware_info.get('cpus_fisicas', 'N/A')}")
            conteudo.append(f"- **CPUs L√≥gicas:** {hardware_info.get('cpus_logicas', 'N/A')}")
            conteudo.append(f"- **Uso CPU (sistema):** {hardware_info.get('cpu_uso_%', 'N/A')}%")
            conteudo.append(f"- **Uso CPU (processo):** {hardware_info.get('cpu_uso_processo_%', 'N/A')}%")
            
            # Se√ß√£o RAM
            conteudo.append("\n### Mem√≥ria RAM")
            conteudo.append(f"- **Total:** {hardware_info.get('mem_total_gb', 'N/A')} GB")
            conteudo.append(f"- **Dispon√≠vel:** {hardware_info.get('mem_disponivel_gb', 'N/A')} GB")
            conteudo.append(f"- **Em Uso:** {hardware_info.get('mem_usada_gb', 'N/A')} GB ({hardware_info.get('mem_uso_%', 'N/A')}%)")
            
            # Se√ß√£o Disco
            conteudo.append("\n### Disco")
            conteudo.append(f"- **Uso:** {hardware_info.get('disco_uso_%', 'N/A')}%")
            
            # Se√ß√£o GPU (nova estrutura)
            gpu_info = hardware_info.get('gpu', {})
            if gpu_info:
                conteudo.append("\n### GPU")
                if gpu_info.get('disponivel', False):
                    conteudo.append(f"- **Total GPUs:** {gpu_info.get('total_gpus', 0)}")
                    gpus = gpu_info.get('gpus', [])
                    for gpu in gpus:
                        if 'erro' in gpu:
                            conteudo.append(f"- **GPU[{gpu['idx']}]:** Erro: {gpu['erro']}")
                        else:
                            conteudo.append(f"- **GPU[{gpu.get('idx', '?')}]:** {gpu.get('nome', 'N/A')}")
                            conteudo.append(f"  - Mem√≥ria Total: {gpu.get('mem_total_gb', 'N/A')} GB")
                            conteudo.append(f"  - Mem√≥ria Reservada: {gpu.get('mem_reservada_gb', 'N/A')} GB")
                            conteudo.append(f"  - Mem√≥ria Alocada: {gpu.get('mem_alocada_gb', 'N/A')} GB")
                            conteudo.append(f"  - Pico Reservado: {gpu.get('mem_max_reservada_gb', 'N/A')} GB")
                            conteudo.append(f"  - Compute Capability: {gpu.get('compute_capability', 'N/A')}")
                else:
                    motivo = gpu_info.get('motivo', 'N√£o dispon√≠vel')
                    conteudo.append(f"- **Status:** {motivo}")
        
        # 3. Dataset
        if dataset_stats:
            conteudo.append("\n## 3. Estat√≠sticas do Dataset")
            conteudo.append(f"- **Total Registros Treino:** {dataset_stats.get('treino_len', 'N/A')}")
            conteudo.append(f"- **Total Registros Valida√ß√£o:** {dataset_stats.get('validacao_len', 'N/A')}")
            
            if 'doc_stats' in dataset_stats:
                ds = dataset_stats['doc_stats']
                conteudo.append("\n### Estat√≠sticas do Documento (Sem Prompt)")
                conteudo.append(f"- **Template (Base):** {ds.get('template_tokens', 0)} tokens")
                conteudo.append(f"- **Documento (M√©dia):** {ds.get('avg_doc_tokens', 0)} tokens")
                conteudo.append(f"- **Documento (Max):** {ds.get('max_doc_tokens', 0)} tokens")

            if 'token_stats' in dataset_stats:
                # Caso antigo (compatibilidade) ou se n√£o tiver estrutura detalhada
                # (Se o token_stats for o dicion√°rio principal, o c√≥digo anterior j√° pega)
                # O c√≥digo atual de collect_returns retorna um dict {split: stats}
                # O dataset_stats passado aqui pode ser apenas o 'train' stats se extra√≠do
                # Mas geralmente dataset_stats cont√©m {'treino_len', ...} e talvez 'token_stats'.
                # Vamos ajustar para iterar splits se existirem
                pass # Tratado acima se implementado na chamada, mas aqui assume estrutura plana
                
            # Exibe stats por split se available (formato novo)
            for split in ['train', 'eval', 'test']:
                if split in dataset_stats and isinstance(dataset_stats[split], dict):
                    s = dataset_stats[split]
                    conteudo.append(f"\n### Distribui√ß√£o de Tokens - {split.upper()}")
                    conteudo.append(f"- **M√©dia:** {s.get('avg_tokens', 0)}")
                    conteudo.append(f"- **M√≠n/M√°x:** {s.get('min_tokens', 0)} / {s.get('max_tokens', 0)}")
                    
                    if 'doc_stats' in s:
                         ds = s['doc_stats']
                         conteudo.append(f"- **Input Texto (M√©dia):** {ds.get('avg_doc_tokens', 0)} (excl. {ds.get('template_tokens', 0)} do template)")
                         conteudo.append(f"- **Input Texto (Max):** {ds.get('max_doc_tokens', 0)}")

        # 4. Treinamento
        if train_stats:
            conteudo.append("\n## 4. Resultados do Treinamento")
            
            # Tabela de m√©tricas principais
            conteudo.append("| M√©trica | Valor |")
            conteudo.append("|---|---|")
            
            # M√©tricas comuns do TRL/Transformers
            metrics_map = {
                "train_runtime": "Tempo Total (s)",
                "train_samples_per_second": "Amostras/seg",
                "total_flos": "Total FLOS",
                "train_loss": "Loss Final (Treino)",
                "epoch": "√âpocas Conclu√≠das"
            }
            
            for k, v in train_stats.items():
                if k in metrics_map:
                    val = f"{v:.4f}" if isinstance(v, float) else str(v)
                    conteudo.append(f"| {metrics_map[k]} | {val} |")
                elif k.startswith("train_") or k == "loss":
                     # Outras m√©tricas de treino
                     pass
            
            # Adiciona outras m√©tricas importantes se presentes
            if 'global_step' in train_stats:
                conteudo.append(f"| Steps Totais | {train_stats['global_step']} |")
                
            # Mem√≥ria GPU se dispon√≠vel (no train_stats customizado)
            if 'mem_gpu_before' in train_stats:
                conteudo.append("\n### Mem√≥ria GPU")
                conteudo.append("**Antes:**")
                conteudo.append("```json")
                conteudo.append(json.dumps(train_stats['mem_gpu_before'], indent=2, ensure_ascii=False))
                conteudo.append("```")
                
            if 'mem_gpu_after' in train_stats:
                conteudo.append("**Depois:**")
                conteudo.append("```json")
                conteudo.append(json.dumps(train_stats['mem_gpu_after'], indent=2, ensure_ascii=False))
                conteudo.append("```")

        texto_final = "\n".join(conteudo)

        if print_only:
             print("\n" + "="*80)
             print("üìÑ PR√âVIA DO RELAT√ìRIO DE TREINAMENTO")
             print("="*80)
             print(texto_final)
             print("="*80 + "\n")
             return ""
        
        # Modo de grava√ß√£o
        if not os.path.isdir(self.output_dir):
            raise FileNotFoundError(f"Erro: Pasta do modelo treinado n√£o encontrada: '{self.output_dir}'. N√£o √© poss√≠vel salvar o relat√≥rio.")

        # Cria diret√≥rio de relat√≥rio (agora garantido que o pai existe)
        os.makedirs(self.report_dir, exist_ok=True)
            
        # Salva arquivo
        with open(self.report_file, "w", encoding="utf-8") as f:
            f.write(texto_final)
            
        print(f"\nüìù Relat√≥rio gerado em: {self.report_file}")
        return self.report_file
