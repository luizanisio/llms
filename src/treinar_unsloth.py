#!/usr/bin/env python3

"""
Autor: Luiz An√≠sio
Fonte: https://github.com/luizanisio/llms/tree/main/src

Treinar Gemma‚Äë3, Deepseek, Llhama, Qwen usando Unsloth 
        + TRL‚ÄëSFTTrainer de forma configur√°vel por yaml.

Uso:
    python treinar_unsloth.py CONFIG.yaml [--debug]

* Se o YAML indicado n√£o existir, um template √© criado e o script
  termina ‚Äî voc√™ revisa os valores e executa novamente.
* O c√≥digo utilizar√° automaticamente todas as GPUs dispon√≠veis,
  sendo gerenciado pelo ambiente do sistema operacional.
  Para isolar as GPUs que ser√£o usadas, defina a vari√°vel de ambiente para o tensorflow CUDA_VISIBLE_DEVICES = <IDs das GPUs>.
  Exemplo: export CUDA_VISIBLE_DEVICES=0,1,2  (no Linux para utilizar as GPUs 0, 1 e 2)
* O par√¢metro **--debug** ativa modo de debug que carrega e apresenta
  a estrutura do dataset e configura√ß√µes importantes sem executar treino.
* O par√¢metro **--modelo N** executa predi√ß√µes em N exemplos do dataset.

**FUNCIONALIDADE DE CHECKPOINTS:**
* O treinamento verifica automaticamente por checkpoints existentes na pasta
  output_dir/chkpt e tenta continuar do √∫ltimo checkpoint v√°lido.
* Se houver erro ao carregar checkpoint (mudan√ßa de par√¢metros), o treinamento
  reinicia do zero mas preserva o modelo LoRA j√° treinado.
* Use resume_from_checkpoint: false no YAML para desabilitar checkpoints.

Exemplo de YAML gerado automaticamente:
```yaml
dataset_train_path: "../dataset/data/dados_unificados_sm_treino.parquet"
train_prompt_col: "messages"
base_model_name: "unsloth/gemma-3-12b-it-unsloth-bnb-4bit"
output_dir: "../modelos/gemma-3-12b-refleg20k-v01"
batch_size: 2
grad_batch_size: 5
num_train_epochs: 1
max_seq_length: 4096
lora_r: 8
save_checkpoints: True
resume_from_checkpoint: True
dataset_eval_path: ""     # opcional
```
"""

import argparse
from cmath import inf
import os, time, json
import sys
import traceback
from typing import Any, Dict
import dataclasses
import yaml
import torch

# Desabilita compila√ß√£o din√¢mica (Dynamo/Inductor) para evitar erros de falta de compilador C
os.environ["TORCH_COMPILE_DISABLE"] = "1"
try:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
    torch._dynamo.config.disable = True
except ImportError:
    pass
import pandas as pd

# Configura√ß√£o de path para permitir execu√ß√£o de qualquer diret√≥rio
# Detecta o diret√≥rio src automaticamente a partir da localiza√ß√£o deste arquivo
_SRC_DIR = os.path.dirname(os.path.abspath(__file__))
if _SRC_DIR not in sys.path:
    sys.path.insert(0, _SRC_DIR)

# Sistema de logging centralizado
from treinar_unsloth_logging import get_logger, configurar_logging, log_separador, log_bloco
from treinar_unsloth_monitor import MonitorRecursos

try:
    from datasets import Dataset
except ImportError:
    print("Erro: O pacote 'datasets' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install datasets")
    sys.exit(1)
try:
    from unsloth import FastModel
except ImportError:
    print("Erro: O pacote 'unsloth' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install unsloth")
    sys.exit(1)
try:
    from unsloth.chat_templates import get_chat_template, CHAT_TEMPLATES, train_on_responses_only
except ImportError:
    print("Erro: O pacote 'unsloth.chat_templates' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install unsloth")
    sys.exit(1)
try:
    from trl import SFTTrainer, SFTConfig
except ImportError:
    print("Erro: O pacote 'trl' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install trl")
    sys.exit(1)
try:
    from transformers import TrainerCallback
except ImportError:
    print("Erro: O pacote 'transformers' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install transformers")
    sys.exit(1)
try:
    from transformers import GenerationConfig
except ImportError:
    print("Erro: O pacote 'transformers' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install transformers")
    sys.exit(1)
try:
    import numpy as np
except ImportError:
    print("Erro: O pacote 'numpy' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install numpy")
    sys.exit(1)
try:
    from datetime import datetime
except ImportError:
    print("Erro: O pacote 'datetime' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install datetime")
    sys.exit(1)
try:
    from copy import deepcopy
except ImportError:
    print("Erro: O pacote 'copy' n√£o est√° instalado.")
    print("Por favor, instale-o executando: pip install copy")
    sys.exit(1)
from unsloth import FastModel
# from unsloth.chat_templates import get_chat_template, CHAT_TEMPLATES, train_on_responses_only
from trl import SFTTrainer, SFTConfig
from transformers import TrainerCallback
from transformers import GenerationConfig
import numpy as np
from datetime import datetime
from copy import deepcopy

# Import da nova classe de configura√ß√£o YAML e Gerador de Relat√≥rio
from treinar_unsloth_util import YamlTreinamento, TIPO_ENTRADA_PASTAS, TIPO_ENTRADA_DATASET, calcular_rouge_l
from treinar_unsloth_report import GeradorRelatorio
from treinar_unsloth_chat import TreinarChatTemplate
from util import UtilEnv, Util

# ---------------------------------------------------------------------------
# Logger do m√≥dulo
# ---------------------------------------------------------------------------

logger = get_logger(__name__)

# ---------------------------------------------------------------------------
# utilidades
# ---------------------------------------------------------------------------

def _print_mem(tag: str, incluir_ram: bool = True) -> dict:
    """
    Exibe estat√≠sticas de mem√≥ria (RAM e GPU) para depura√ß√£o r√°pida.
    Utiliza Util.dados_hardware() para obter informa√ß√µes centralizadas.
    
    Args:
        tag: Identificador para o print (ex: "ANTES", "DEPOIS")
        incluir_ram: Se True, tamb√©m exibe informa√ß√µes de RAM
        
    Returns:
        Dict com informa√ß√µes completas de hardware (CPU, RAM, GPU)
    """
    try:
        hardware = Util.dados_hardware(incluir_gpu=True)
    except Exception as e:
        logger.warning(f"[{tag}] Erro ao obter dados de hardware: {e}")
        return {}
    
    # Exibe informa√ß√µes de RAM
    if incluir_ram:
        ram_total = hardware.get('mem_total_gb', 0)
        ram_usada = hardware.get('mem_usada_gb', 0)
        ram_disp = hardware.get('mem_disponivel_gb', 0)
        ram_uso = hardware.get('mem_uso_%', 0)
        logger.info(f"[{tag}] RAM | usada: {ram_usada:.2f} GB / total: {ram_total:.2f} GB ({ram_uso:.1f}%) | dispon√≠vel: {ram_disp:.2f} GB")
    
    # Exibe informa√ß√µes de GPU
    gpu_info = hardware.get('gpu', {})
    if not gpu_info.get('disponivel', False):
        motivo = gpu_info.get('motivo', 'CUDA n√£o dispon√≠vel')
        logger.info(f"[{tag}] GPU | {motivo}")
    else:
        gpus = gpu_info.get('gpus', [])
        for gpu in gpus:
            if 'erro' in gpu:
                logger.warning(f"[{tag}] GPU[{gpu['idx']}] Erro: {gpu['erro']}")
            else:
                nome = gpu.get('nome', 'N/A')
                mem_total = gpu.get('mem_total_gb', 0)
                mem_reservada = gpu.get('mem_reservada_gb', 0)
                mem_max_reservada = gpu.get('mem_max_reservada_gb', 0)
                mem_alocada = gpu.get('mem_alocada_gb', 0)
                logger.info(f"[{tag}] GPU[{gpu['idx']}] {nome} | reservada: {mem_max_reservada:.2f} GB / total: {mem_total:.2f} GB | alocada: {mem_alocada:.2f} GB")
    
    return hardware

class JsonLoggerCallback(TrainerCallback):
    """Callback para registrar m√©tricas de treinamento em formato JSONL."""
    
    def __init__(self, path):
        self.path = path
        # zera o arquivo no in√≠cio
        open(self.path, "w").close()

    # logs = {'loss': ‚Ä¶}  ou  {'eval_loss': ‚Ä¶}
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            logs["step"]  = state.global_step
            logs["epoch"] = state.epoch
            logs["time"]  = time.time()
            with open(self.path, "a") as fp:
                fp.write(json.dumps(logs, ensure_ascii=False) + "\n")

    # garante que tamb√©m pegamos o dicion√°rio completo emitido
    # pelo m√©todo evaluate() externo, se voc√™ cham√°-lo no fim
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        self.on_log(args, state, control, metrics)


class HardwareMetricsCallback(TrainerCallback):
    """
    Callback para registrar m√©tricas cont√≠nuas de hardware durante o treinamento.
    
    Registra: CPU (uso %), RAM (usada/dispon√≠vel GB), Disco (uso %), GPU (mem√≥ria alocada/reservada GB)
    As m√©tricas s√£o salvas em arquivo JSONL separado para an√°lise posterior.
    """
    
    def __init__(self, output_dir: str, intervalo_steps: int = 10):
        """
        Args:
            output_dir: Diret√≥rio onde salvar o arquivo de m√©tricas
            intervalo_steps: Intervalo de steps entre registros (evita overhead excessivo)
        """
        self.output_dir = output_dir
        self.intervalo_steps = max(1, intervalo_steps)
        self.metrics_file = os.path.join(output_dir, "treinamento", "hardware_metrics.jsonl")
        self._ultimo_step = -1
        
        # Cria diret√≥rio e arquivo
        os.makedirs(os.path.dirname(self.metrics_file), exist_ok=True)
        open(self.metrics_file, "w").close()  # Limpa arquivo anterior
        
    def _registrar_metricas(self, step: int, epoch: float, fase: str = "train"):
        """Registra m√©tricas de hardware no arquivo JSONL."""
        try:
            hardware = Util.dados_hardware(incluir_gpu=True)
            
            registro = {
                "timestamp": time.time(),
                "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "step": step,
                "epoch": round(epoch, 4) if epoch else 0,
                "fase": fase,
                # CPU
                "cpu_uso_%": hardware.get("cpu_uso_%", 0),
                "cpu_uso_processo_%": hardware.get("cpu_uso_processo_%", 0),
                # RAM
                "ram_usada_gb": hardware.get("mem_usada_gb", 0),
                "ram_disponivel_gb": hardware.get("mem_disponivel_gb", 0),
                "ram_uso_%": hardware.get("mem_uso_%", 0),
                # Disco
                "disco_uso_%": hardware.get("disco_uso_%", 0),
            }
            
            # GPU (pode ter m√∫ltiplas)
            gpu_info = hardware.get("gpu", {})
            if gpu_info.get("disponivel", False):
                gpus = gpu_info.get("gpus", [])
                for gpu in gpus:
                    idx = gpu.get("idx", 0)
                    registro[f"gpu{idx}_reservada_gb"] = gpu.get("mem_reservada_gb", 0)
                    registro[f"gpu{idx}_alocada_gb"] = gpu.get("mem_alocada_gb", 0)
                    registro[f"gpu{idx}_max_reservada_gb"] = gpu.get("mem_max_reservada_gb", 0)
            
            with open(self.metrics_file, "a") as fp:
                fp.write(json.dumps(registro, ensure_ascii=False) + "\n")
                
        except Exception as e:
            # N√£o interrompe o treinamento por erro de m√©tricas
            pass
    
    def on_step_end(self, args, state, control, **kwargs):
        """Registra m√©tricas a cada N steps."""
        if state.global_step % self.intervalo_steps == 0 and state.global_step != self._ultimo_step:
            self._ultimo_step = state.global_step
            self._registrar_metricas(state.global_step, state.epoch, "train")
    
    def on_evaluate(self, args, state, control, **kwargs):
        """Registra m√©tricas durante avalia√ß√£o."""
        self._registrar_metricas(state.global_step, state.epoch, "eval")
    
    def on_train_begin(self, args, state, control, **kwargs):
        """Registra m√©tricas no in√≠cio do treinamento."""
        self._registrar_metricas(0, 0, "train_begin")
    
    def on_train_end(self, args, state, control, **kwargs):
        """Registra m√©tricas no final do treinamento."""
        self._registrar_metricas(state.global_step, state.epoch, "train_end")


class MetricsLoggerCallback(TrainerCallback):
    """
    Callback aprimorado para registrar m√©tricas detalhadas de treinamento e valida√ß√£o.
    
    Registra: loss, learning_rate, grad_norm, e m√©tricas de avalia√ß√£o (eval_loss)
    Salva em arquivo JSONL separado com informa√ß√µes adicionais de contexto.
    """
    
    def __init__(self, output_dir: str):
        """
        Args:
            output_dir: Diret√≥rio onde salvar o arquivo de m√©tricas
        """
        self.output_dir = output_dir
        self.metrics_file = os.path.join(output_dir, "treinamento", "training_metrics.jsonl")
        self._train_start_time = None
        self._best_eval_loss = float('inf')
        self._train_losses = []  # Para calcular m√©dia m√≥vel
        
        # Cria diret√≥rio e arquivo
        os.makedirs(os.path.dirname(self.metrics_file), exist_ok=True)
        open(self.metrics_file, "w").close()  # Limpa arquivo anterior
        
    def _registrar(self, registro: dict):
        """Salva registro no arquivo JSONL."""
        try:
            with open(self.metrics_file, "a") as fp:
                fp.write(json.dumps(registro, ensure_ascii=False) + "\n")
        except Exception:
            pass
    
    def on_train_begin(self, args, state, control, **kwargs):
        """Marca in√≠cio do treinamento."""
        self._train_start_time = time.time()
        self._registrar({
            "event": "train_begin",
            "timestamp": self._train_start_time,
            "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "total_steps": state.max_steps,
            "num_epochs": args.num_train_epochs,
            "batch_size": args.per_device_train_batch_size,
            "grad_accum_steps": args.gradient_accumulation_steps,
        })
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """Registra m√©tricas de treinamento a cada log."""
        if not logs:
            return
            
        registro = {
            "event": "log",
            "timestamp": time.time(),
            "step": state.global_step,
            "epoch": round(state.epoch, 4) if state.epoch else 0,
            "elapsed_seconds": time.time() - self._train_start_time if self._train_start_time else 0,
        }
        
        # M√©tricas de treinamento
        if "loss" in logs:
            registro["train_loss"] = round(logs["loss"], 6)
            self._train_losses.append(logs["loss"])
            # M√©dia m√≥vel das √∫ltimas 10 perdas
            if len(self._train_losses) >= 10:
                registro["train_loss_avg_10"] = round(sum(self._train_losses[-10:]) / 10, 6)
        
        if "learning_rate" in logs:
            registro["learning_rate"] = logs["learning_rate"]
            
        if "grad_norm" in logs:
            registro["grad_norm"] = round(logs["grad_norm"], 6)
        
        # M√©tricas de avalia√ß√£o
        if "eval_loss" in logs:
            registro["eval_loss"] = round(logs["eval_loss"], 6)
            if logs["eval_loss"] < self._best_eval_loss:
                self._best_eval_loss = logs["eval_loss"]
                registro["is_best_eval"] = True
        
        # Progresso
        if state.max_steps > 0:
            registro["progress_%"] = round(state.global_step / state.max_steps * 100, 2)
        
        self._registrar(registro)
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """Registra m√©tricas completas de avalia√ß√£o."""
        if not metrics:
            return
            
        registro = {
            "event": "evaluate",
            "timestamp": time.time(),
            "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "step": state.global_step,
            "epoch": round(state.epoch, 4) if state.epoch else 0,
        }
        
        # Copia todas as m√©tricas de avalia√ß√£o
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                registro[key] = round(value, 6) if isinstance(value, float) else value
        
        self._registrar(registro)
    
    def on_train_end(self, args, state, control, **kwargs):
        """Registra resumo final do treinamento."""
        elapsed = time.time() - self._train_start_time if self._train_start_time else 0
        
        registro = {
            "event": "train_end",
            "timestamp": time.time(),
            "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "total_steps": state.global_step,
            "final_epoch": round(state.epoch, 4) if state.epoch else 0,
            "total_time_seconds": round(elapsed, 2),
            "total_time_formatted": f"{int(elapsed // 3600)}h {int((elapsed % 3600) // 60)}m {int(elapsed % 60)}s",
            "best_eval_loss": round(self._best_eval_loss, 6) if self._best_eval_loss != float('inf') else None,
            "final_train_loss_avg": round(sum(self._train_losses[-10:]) / len(self._train_losses[-10:]), 6) if self._train_losses else None,
        }
        
        self._registrar(registro)

# ---------------------------------------------------------------------------
# Callback para renomear checkpoints com zero-padding
# ---------------------------------------------------------------------------

class CheckpointRenameCallback(TrainerCallback):
    """
    Callback para renomear checkpoints com zero-padding.
    
    Transforma: checkpoint-8 -> checkpoint-00008
    Isso permite ordena√ß√£o alfab√©tica correta e evita confus√£o.
    """
    
    def __init__(self, checkpoint_base_dir: str, padding: int = 5):
        self.checkpoint_base_dir = checkpoint_base_dir
        self.padding = padding
    
    def on_save(self, args, state, control, **kwargs):
        """Renomeia o checkpoint salvo para usar zero-padding."""
        if not state.global_step:
            return
            
        # Caminho esperado do checkpoint original
        original_name = f"checkpoint-{state.global_step}"
        padded_name = f"checkpoint-{state.global_step:0{self.padding}d}"
        
        original_path = os.path.join(self.checkpoint_base_dir, original_name)
        padded_path = os.path.join(self.checkpoint_base_dir, padded_name)
        
        # Se o diret√≥rio original existe e o padded n√£o existe, renomeia
        if os.path.exists(original_path) and not os.path.exists(padded_path):
            try:
                os.rename(original_path, padded_path)
                logger.debug(f"Checkpoint renomeado: {original_name} -> {padded_name}")
            except Exception as e:
                logger.warning(f"Erro ao renomear checkpoint: {e}")
        elif os.path.exists(padded_path):
            # J√° existe com zero-padding, provavelmente foi renomeado anteriormente
            logger.warning(f"Erro ao renomear checkpoint: j√° existe com zero-padding {padded_name}")

# ---------------------------------------------------------------------------
# classe principal
# ---------------------------------------------------------------------------

class LLMsTrainer:
    """Encapsula o fluxo de fine‚Äëtuning de LLMs com LoRA e Unsloth."""

    # Chaves obrigat√≥rias no formato flat (para compatibilidade do m√©todo para_config_flat)
    # REQUIRED_KEYS removido pois valida√ß√£o √© feita nos dataclasses

    def __init__(self, cfg_path: str, force_base: bool = False):
        # Carrega configura√ß√£o YAML
        self._yaml_config = YamlTreinamento(cfg_path)
        
        self.force_base = force_base
        
        # Cria a pasta de sa√≠da se n√£o existir
        os.makedirs(self._yaml_config.modelo.saida, exist_ok=True)
        
        # Carrega modelo e tokenizer
        self.model, self.tokenizer = self._load_model()
        
        # Gerenciador de templates de chat
        self.chat_handler = TreinarChatTemplate(self.tokenizer, self._yaml_config.modelo.base)
        self.tokenizer = self.chat_handler.tokenizer
        
        # Carrega datasets baseado no tipo de entrada
        if self._yaml_config.tipo_entrada == TIPO_ENTRADA_PASTAS:
            # Modo pastas: carrega de arquivos pareados
            self.train_ds = self._load_from_pastas(alvo="treino")
            self.eval_ds = self._load_from_pastas(alvo="validacao")
        else:
            # Modo dataset: carrega de arquivos parquet
            self.train_ds = self._load_split(
                self._yaml_config.dataset.train_file, 
                self._yaml_config.dataset.train_prompt_col, 
                split="treino"
            )
            self.eval_ds = None
            if self._yaml_config.dataset.eval_file:
                self.eval_ds = self._load_split(
                    self._yaml_config.dataset.eval_file,
                    self._yaml_config.dataset.eval_prompt_col or self._yaml_config.dataset.train_prompt_col,
                    split="valida√ß√£o",
                )
        
        # Exibe estat√≠sticas pr√©-treinamento e armazena para relat√≥rio
        ts = self._print_dataset_stats(self.train_ds, "Dataset de Treino")
        es = self._print_dataset_stats(self.eval_ds, "Dataset de Valida√ß√£o") if self.eval_ds and len(self.eval_ds) > 0 else {}
        
        self._dataset_stats = {
            "treino_len": len(self.train_ds),
            "validacao_len": len(self.eval_ds) if self.eval_ds else 0,
            "token_stats": ts
        }

        # Log do primeiro registro
        if len(self.train_ds) > 0:
            self.log_processamento(self.train_ds[0], titulo="Primeiro registro do dataset de treino")
        
        self.save_checkpoints = self._yaml_config.treinamento.save_checkpoints
        self.trainer = None  # Inicializa√ß√£o lazy no m√©todo train()

    # ------------------------- controle no colab ------------------------------
    @classmethod
    def verifica_versao(cls):
        print(f'JsonAnalise carregado corretamente em {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}!')
        
    # ------------------------- configura√ß√£o ------------------------------
    def _validate_cfg(self) -> None:
        """Valida√ß√£o j√° realizada pela YamlTreinamento."""
        pass
    
    def _print_dataset_stats(self, dataset: Dataset, nome: str) -> dict:
        """Exibe estat√≠sticas de tokens do dataset e retorna dict com dados."""
        if dataset is None or len(dataset) == 0:
            print(f"üìä {nome}: vazio")
            return {}
        
        lengths = [len(r.get('input_ids', [])) for r in dataset]
        if not lengths or max(lengths) == 0:
            # Dataset ainda n√£o tokenizado, conta mensagens
            print(f"üìä {nome}: {len(dataset)} registros (n√£o tokenizado)")
            return {}
        
        min_l, max_l = min(lengths), max(lengths)
        avg_l = sum(lengths)/len(lengths)
        
        print(f"üìä {nome}:")
        print(f"   Registros: {len(dataset)}")
        print(f"   Tokens: min={min_l}, max={max_l}, m√©dia={avg_l:.0f}")
        
        # Alerta se houver sequ√™ncias que excedem max_seq_length
        max_seq = self._yaml_config.treinamento.max_seq_length
        excedem = sum(1 for l in lengths if l > max_seq)
        if excedem > 0:
            print(f"   ‚ö†Ô∏è  {excedem} registros excedem max_seq_length={max_seq}")
        
        return {
            "min": min_l,
            "max": max_l,
            "avg": round(avg_l, 1),
            "exceed_max_seq": excedem
        }
    
    def _load_from_pastas(self, alvo: str) -> Dataset:
        """Carrega dataset a partir de pastas usando YamlTreinamento."""
        print(f"[2/6] Carregando dados de pastas (alvo={alvo})...")
        
        # Carrega mensagens usando dataset_manager via YamlTreinamento
        mensagens = self._yaml_config.dataset_manager.carregar_mensagens_de_pastas(alvo=alvo)
        
        if not mensagens:
            print(f"   ‚ö†Ô∏è  Nenhum registro encontrado para alvo='{alvo}'")
            return Dataset.from_list([])
        
        print(f"   Encontrados {len(mensagens)} registros para {alvo}")
        
        # Cria LLMsDataset a partir dos dados em mem√≥ria
        dataset_loader = LLMsDataset(
            data=mensagens,
            tokenizer=self.tokenizer,
            max_seq_length=self._yaml_config.treinamento.max_seq_length
        )
        
        return dataset_loader.dataset
        
    # ------------------------- modelo ------------------------------------
    def _load_model(self):
        print("[1/6] Carregando modelo base‚Ä¶")
        # Carrega configura√ß√£o de bits
        nbits = self._yaml_config.treinamento.nbits
        
        lora_ok = False
        
        if not self.force_base:
            # Verifica se existe modelo LoRA j√° treinado
            lora_model_path = self._yaml_config.modelo.saida
            arq_lora = os.path.join(lora_model_path, 'adapter_config.json')
            arq_model = os.path.join(lora_model_path, 'adapter_model.safetensors')
            
            # Verifica se √© um modelo LoRA completo (n√£o apenas um checkpoint)
            is_trained_lora = (os.path.exists(arq_lora) and 
                            (os.path.exists(arq_model) or 
                            os.path.exists(os.path.join(lora_model_path, 'pytorch_model.bin'))))
            
            if is_trained_lora:
                print(f'üîÑ Carregando modelo LoRA j√° treinado de {lora_model_path}...')
                try:
                    # Carrega o modelo LoRA j√° treinado diretamente
                    model, tokenizer = FastModel.from_pretrained(
                        model_name=lora_model_path,  # Carrega da pasta do modelo treinado
                        max_seq_length=self._yaml_config.treinamento.max_seq_length,
                        load_in_4bit=nbits == 4,
                        load_in_8bit=nbits == 8,
                        device_map="auto",
                    )
                    print(f'‚úÖ Modelo LoRA treinado carregado com sucesso!')
                    lora_ok = True
                    
                    # Log de informa√ß√µes do modelo carregado
                    self.log_processamento(f"Modelo LoRA carregado de: {lora_model_path}", "LORA_LOADED")
                    
                except Exception as e:
                    print(f'‚ùå Erro ao carregar modelo LoRA treinado: {e}')
                    traceback.print_exc()
                    print('Tentando carregar modelo base e aplicar LoRA...')
                    time.sleep(2)
        else:
             print(f'‚ÑπÔ∏è  Op√ß√£o --base ativada: Ignorando busca por modelo LoRA treinado.')
        
        # Se n√£o conseguiu carregar o LoRA ou n√£o existe ou force_base=True, carrega modelo base
        if not lora_ok:
            print(f'üîÑ Carregando modelo base: {self._yaml_config.modelo.base}...')
            model, tokenizer = FastModel.from_pretrained(
                model_name=self._yaml_config.modelo.base,
                max_seq_length=self._yaml_config.treinamento.max_seq_length,
                load_in_4bit=nbits == 4,
                load_in_8bit=nbits == 8,
                full_finetuning=self._yaml_config.lora.r in (0,None,False) or self.force_base # Se force_base, n√£o prepara para finetuning
            )
            
            # Se usar LoRA, aplica as configura√ß√µes (Exceto se force_base=True)
            if not self.force_base and self._yaml_config.lora.r not in (0,None,False):
                print(f'üîÑ Aplicando LoRA r={self._yaml_config.lora.r} ao modelo base ...')
                model = FastModel.get_peft_model(
                    model,
                    finetune_vision_layers=False,
                    finetune_language_layers=True,
                    finetune_attention_modules=True,
                    finetune_mlp_modules=True,
                    r=self._yaml_config.lora.r,
                    lora_alpha=self._yaml_config.lora.alpha,
                    lora_dropout=self._yaml_config.lora.dropout,
                    bias="none",
                    random_state=3407,
                    device_map="auto",
                )
            elif self.force_base:
                print(f'‚ÑπÔ∏è  Op√ß√£o --base ativada: N√£o aplicando adaptadores LoRA.')
        # Template agora √© aplicado pelo TreinarChatTemplate ap√≥s o carregamento
        model.print_trainable_parameters()
        
        # Log detalhado do modelo carregado
        model_type = type(model).__name__
        is_peft_model = hasattr(model, 'peft_config') or hasattr(model, 'base_model')
        
        print(f"\nüìä MODELO CARREGADO:")
        print(f"  - Tipo: {model_type}")
        print(f"  - √â modelo PEFT: {is_peft_model}")
        print(f"  - LoRA carregado: {lora_ok}")
        
        if is_peft_model:
            try:
                if hasattr(model, 'peft_config'):
                    peft_configs = model.peft_config
                    print(f"  - Configura√ß√µes PEFT: {list(peft_configs.keys())}")
                    for adapter_name, config in peft_configs.items():
                        print(f"    * {adapter_name}: r={getattr(config, 'r', 'N/A')}, alpha={getattr(config, 'lora_alpha', 'N/A')}")
                elif hasattr(model, 'base_model'):
                    print(f"  - Modelo base: {type(model.base_model).__name__}")
            except Exception as e:
                print(f"  - Erro ao obter detalhes PEFT: {e}")
        
        self.log_processamento(self._yaml_config._raw_config, titulo="Configura√ß√£o do treinamento")
        self.log_processamento(str(model), titulo="Resumo do modelo")
        self.log_processamento(f"Tipo do modelo: {model_type} | PEFT: {is_peft_model} | LoRA OK: {lora_ok}", titulo="Status do modelo")
        self.log_processamento(tokenizer.chat_template, titulo="Template do tokenizer")
        return model, tokenizer


    # ------------------------- dados -------------------------------------
    def _load_split(self, parquet_path: str, prompt_col: str, *, split: str) -> Dataset:
        """Carrega dataset usando classe LLMsDataset para padroniza√ß√£o."""
        print(f"[2/6] Lendo {split} de {parquet_path}‚Ä¶")
        
        # cria inst√¢ncia da classe LLMsDataset com detec√ß√£o autom√°tica do template
        dataset_loader = LLMsDataset(
            path=parquet_path,
            prompt_col=prompt_col,
            tokenizer=self.tokenizer,
            max_seq_length=self._yaml_config.treinamento.max_seq_length
        )
        
        # obt√©m dataset processado
        print(f" - {split} carregado com {len(dataset_loader.dataset)} registros")
        
        return dataset_loader.dataset

    @classmethod
    def debug_info(cls, cfg_path: str):
        """Exibe informa√ß√µes detalhadas de debug sobre configura√ß√£o e datasets."""
        print("="*80)
        print(">> MODO INFO / DEBUG - INFORMA√á√ïES DE CONFIGURA√á√ÉO E DATASET")
        print("="*80)
        
        # Carrega configura√ß√£o usando YamlTreinamento
        try:
            yaml_config = YamlTreinamento(cfg_path, validar_caminhos=False)
        except Exception as e:
            print(f"\n‚ùå Erro ao carregar YAML: {e}")
            import traceback
            traceback.print_exc()
            return
        
        # Mostra informa√ß√µes do YamlTreinamento
        print(f"\n{yaml_config.info()}")
        
        # configura√ß√£o estruturada
        print("\nüìã CONFIGURA√á√ÉO ESTRUTURADA:")
        config_dict = {
             "modelo": dataclasses.asdict(yaml_config.modelo),
             "treinamento": dataclasses.asdict(yaml_config.treinamento),
             "lora": dataclasses.asdict(yaml_config.lora),
             "dataset": dataclasses.asdict(yaml_config.dataset) if yaml_config.dataset else None,
             "formatos": dataclasses.asdict(yaml_config.formatos)
        }
        print(json.dumps(config_dict, indent=2, ensure_ascii=False, default=str))

        # carrega o tokenizer para chat_template
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(yaml_config.modelo.base, use_fast=True)
            print(f"\n‚úÖ Tokenizer carregado com sucesso")
        except ImportError:
            print(f"\n‚ùå Transformers n√£o dispon√≠vel")
            return
        except Exception as e:
            print(f"\n‚ùå Erro ao carregar tokenizer: {e}")
            return
        
        # informa√ß√µes do modelo
        template_type = LLMsDataset.template_com_type(tokenizer)
        print(f"\nü§ñ MODELO:")
        print(f"  - Nome: {yaml_config.modelo.base}")
        print(f"  - LoRA r: {yaml_config.lora.r}")
        print(f"  - Max seq length: {yaml_config.treinamento.max_seq_length}")
        print(f"  - Template com type: {template_type}")
        
        # Verifica se existe modelo LoRA treinado
        lora_model_path = yaml_config.modelo.saida
        arq_lora = os.path.join(lora_model_path, 'adapter_config.json')
        arq_model = os.path.join(lora_model_path, 'adapter_model.safetensors')
        pytorch_model = os.path.join(lora_model_path, 'pytorch_model.bin')
        
        print(f"\nüîß VERIFICA√á√ÉO DE MODELO TREINADO:")
        print(f"  - Pasta do modelo: {lora_model_path}")
        print(f"  - adapter_config.json existe: {os.path.exists(arq_lora)}")
        print(f"  - adapter_model.safetensors existe: {os.path.exists(arq_model)}")
        print(f"  - pytorch_model.bin existe: {os.path.exists(pytorch_model)}")
        
        if os.path.exists(arq_lora):
            try:
                with open(arq_lora, 'r') as f:
                    lora_config = json.load(f)
                print(f"  - Configura√ß√£o LoRA: r={lora_config.get('r', 'N/A')}, alpha={lora_config.get('lora_alpha', 'N/A')}")
            except:
                print(f"  - Erro ao ler configura√ß√£o LoRA")
        
        is_trained_lora = (os.path.exists(arq_lora) and 
                          (os.path.exists(arq_model) or os.path.exists(pytorch_model)))
        print(f"  - Modelo LoRA completo detectado: {is_trained_lora}")
        
        if is_trained_lora:
            print(f"  ‚úÖ O modelo ser√° carregado com LoRA treinado")
        elif yaml_config.lora.r not in (0, None, False):
            print(f"  üîÑ Ser√° aplicado novo LoRA ao modelo base")
        else:
            print(f"  üìÑ Ser√° usado modelo base sem LoRA")
        
        # Modo pastas: mostra arquivos pareados
        if yaml_config.tipo_entrada == TIPO_ENTRADA_PASTAS:
            print(f"\nüìÅ MODO PASTAS - ARQUIVOS PAREADOS:")
            try:
                pares = yaml_config.dataset_manager.parear_arquivos()
                print(f"  - Total de pares: {len(pares)}")
                if pares:
                    print(f"  - Primeiros 3 pares:")
                    for par in pares[:3]:
                        print(f"    * {par.get('id', 'N/A')}")
                
                # Carrega divis√£o se existir
                divisao = yaml_config.dataset_manager.carregar_ou_criar_divisao()
                if not divisao.empty:
                    contagem = divisao['alvo'].value_counts()
                    total = len(divisao)
                    print(f"\n  üìä Divis√£o de dados (total = {total}):")
                    for alvo, qtd in contagem.items():
                        print(f"    - {alvo}: {qtd}")
                
                # Testa carregamento de mensagens
                print(f"\n  üîÑ Carregando amostras de mensagens...")
                
                msgs_treino = yaml_config.dataset_manager.carregar_mensagens_de_pastas(alvo="treino")
                print(f"    - Mensagens de treino: {len(msgs_treino)}")
                yaml_config.dataset_manager.mostrar_exemplo("Amostra Treino", msgs_treino)

                # Mostra tamb√©m teste e valida√ß√£o se existirem
                if not yaml_config.dataset_manager.carregar_ou_criar_divisao().empty:
                    msgs_teste = yaml_config.dataset_manager.carregar_mensagens_de_pastas(alvo="teste")
                    if msgs_teste:
                        print(f"    - Mensagens de teste: {len(msgs_teste)}")
                        yaml_config.dataset_manager.mostrar_exemplo("Amostra Teste", msgs_teste)
                    
                    msgs_val = yaml_config.dataset_manager.carregar_mensagens_de_pastas(alvo="validacao")

                    if msgs_val:
                        print(f"    - Mensagens de valida√ß√£o: {len(msgs_val)}")
                        yaml_config.dataset_manager.mostrar_exemplo("Amostra Valida√ß√£o", msgs_val)
                    

            except Exception as e:
                print(f"  ‚ùå Erro ao processar pastas: {e}")
                import traceback
                traceback.print_exc()
        else:
            # Modo dataset: carrega de arquivo
            if yaml_config.dataset and yaml_config.dataset.train_file:
                try:
                    train_loader = LLMsDataset(
                        path=yaml_config.dataset.train_file,
                        prompt_col=yaml_config.dataset.train_prompt_col,
                        tokenizer=tokenizer,
                        max_seq_length=yaml_config.treinamento.max_seq_length
                    )
                    train_stats = train_loader.get_stats()
                    print(f"\nüìä DATASET DE TREINO:")
                    print(f"  - Registros: {train_stats['total_registros']}")
                    print(f"  - Colunas: {train_stats['colunas']}")
                    print(f"  - Formato: {train_stats['formato_arquivo']}")
                    print(f"  - Caminho: {train_stats['caminho']}")
                except Exception as e:
                    print(f"\n‚ùå Erro ao carregar dataset de treino: {e}")
        
        # informa√ß√µes de checkpoints
        print(f"\nüíæ CHECKPOINT INFO:")
        checkpoint_dir = os.path.join(yaml_config.modelo.saida, "chkpt")
        resume_enabled = yaml_config.treinamento.resume_from_checkpoint
        print(f"  - Resume from checkpoint: {resume_enabled}")
        print(f"  - Checkpoint directory: {checkpoint_dir}")
        
        if os.path.exists(checkpoint_dir):
            checkpoints = []
            for item in os.listdir(checkpoint_dir):
                item_path = os.path.join(checkpoint_dir, item)
                if os.path.isdir(item_path) and item.startswith("checkpoint-"):
                    try:
                        step_num = int(item.split("-")[1])
                        checkpoints.append((step_num, item))
                    except (IndexError, ValueError):
                        continue
            
            if checkpoints:
                checkpoints.sort(key=lambda x: x[0])
                print(f"  - Checkpoints encontrados: {len(checkpoints)}")
                for step, name in checkpoints[-3:]:  # mostra os 3 mais recentes
                    print(f"    * {name} (step {step})")
            else:
                print(f"  - Nenhum checkpoint encontrado")
        else:
            print(f"  - Diret√≥rio de checkpoints n√£o existe")
        
        # informa√ß√µes de hardware (RAM e GPU)
        print(f"\nüéÆ HARDWARE INFO:")
        try:
            hardware = _print_mem("DEBUG")
        except Exception as e:
            print(f"  ‚ùå Erro ao obter info de hardware: {e}")
            hardware = {}
        
        gerador = GeradorRelatorio(yaml_config)
        gerador.gerar_relatorio(
            dataset_stats=None, 
            train_stats=None,
            hardware_info=hardware,
            print_only=True
        )
        
        print("\n" + "="*80)
        print("‚úÖ INFO / DEBUG COMPLETO - CONFIGURA√á√ÉO E DATASETS VALIDADOS")
        print("="*80)


    # ------------------------- trainer -----------------------------------
    def _build_trainer(self) -> SFTTrainer:
        print("[3/6] Configurando trainer‚Ä¶")
        
        # === Formata√ß√£o do Dataset (garante coluna 'text') ===
        # Define num_proc seguro
        import os
        n_proc = max(1, (os.cpu_count() or 2) // 2)

        if "text" not in self.train_ds.column_names:
            self.train_ds = self.chat_handler.formatar_dataset_coluna_text(self.train_ds, num_proc=n_proc)
            
        if self.eval_ds and "text" not in self.eval_ds.column_names:
            self.eval_ds = self.chat_handler.formatar_dataset_coluna_text(self.eval_ds, num_proc=n_proc)
            
        # Verifica a integridade da formata√ß√£o para DEBUG
        self.chat_handler.verificar_dataset_formatado(self.train_ds)
        # =====================================================

        total_examples = len(self.train_ds)
        # cfg = self.cfg (removido)
        
        treino_cfg = self._yaml_config.treinamento
        
        eval_steps = treino_cfg.eval_steps
        n_gpus = max(torch.cuda.device_count(), 1)
        
        # percentual do dataset
        if self.eval_ds and isinstance(eval_steps, str) and eval_steps.endswith('%'):
            try:
                eval_steps_val = int(eval_steps.replace('%', '').strip())
                if eval_steps_val >= 1:
                   _st = treino_cfg.grad_batch_size * treino_cfg.batch_size * n_gpus
                   eval_steps = int((eval_steps_val/100) * (total_examples / _st))
                else:
                   eval_steps = None
            except:
                eval_steps = None
        
        if eval_steps is None or not self.eval_ds:
            eval_steps = 0  
            
        if eval_steps == 0 and self.eval_ds:
             # Fallback c√°lculo autom√°tico se n√£o definido
             _st = treino_cfg.grad_batch_size * treino_cfg.batch_size * n_gpus
             eval_steps = max(1, int((total_examples / 100) / _st))

        if self.eval_ds and eval_steps > 0:
           print(f' - avaliando a cada {eval_steps} steps...')
        
        log_steps = eval_steps if isinstance(eval_steps, int) and eval_steps > 0 else 50
        
        if self.save_checkpoints:
            print(f' - gravando checkpoints a cada {log_steps} steps')
        
        # Log train_on_responses_only
        if treino_cfg.train_on_responses_only:
            print(f' - train_on_responses_only ATIVADO (treina apenas nas respostas do assistant)')

        # Configura√ß√£o de argumentos de treino
        # Nota: Usamos TrainingArguments padr√£o ou SFTConfig se dispon√≠vel no unsloth
        # Para garantir compatibilidade, usamos TrainingArguments que √© base
        from trl import SFTConfig
        
        args = SFTConfig(
            per_device_train_batch_size=treino_cfg.batch_size,
            gradient_accumulation_steps=treino_cfg.grad_batch_size,
            warmup_steps=treino_cfg.warmup_steps,
            num_train_epochs=treino_cfg.epochs,
            learning_rate=treino_cfg.learning_rate,
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=1,
            optim=treino_cfg.optim,
            weight_decay=treino_cfg.weight_decay,
            lr_scheduler_type=treino_cfg.lr_scheduler_type,
            seed=treino_cfg.seed,
            output_dir=os.path.join(self._yaml_config.modelo.saida, "chkpt"),  # checkpoints em subpasta
            save_strategy="steps" if self.save_checkpoints else "no",
            save_steps=log_steps if self.save_checkpoints else 0,
            eval_strategy="steps" if self.eval_ds and eval_steps > 0 else "no",
            eval_steps=eval_steps if self.eval_ds and eval_steps > 0 else None,
            load_best_model_at_end=True if self.eval_ds and eval_steps > 0 else False,
            report_to="none", 
            gradient_checkpointing="unsloth", 
            remove_unused_columns=False,
            dataloader_drop_last=False,
            dataset_text_field="text", # usamos a coluna 'text' formatada
            max_seq_length=treino_cfg.max_seq_length, 
            dataset_num_proc=2,
            packing=False,
            per_device_eval_batch_size=1,     # For√ßa batch 1 na valida√ß√£o para economizar VRAM
            eval_accumulation_steps=1,        # Descarrega logits da GPU para CPU a cada passo
        )

        trainer = SFTTrainer(
            model=self.model,
            tokenizer=self.tokenizer,
            train_dataset=self.train_ds,
            eval_dataset=self.eval_ds,
            args=args,
        )
        
        # Aplica train_on_responses_only se configurado
        # Aplica train_on_responses_only se configurado
        if treino_cfg.train_on_responses_only:
            self.trainer = self.chat_handler.aplicar_train_on_responses_only(self.trainer)
        
        # Configura diret√≥rio de sa√≠da
        output_dir = self._yaml_config.modelo.saida
        os.makedirs(output_dir, exist_ok=True)
        
        # === CALLBACKS DE M√âTRICAS ===
        
        # 1. JsonLogger (m√©tricas brutas em metrics_stream.jsonl)
        jsonl = os.path.join(output_dir, "metrics_stream.jsonl")
        if os.path.isfile(jsonl):
            os.remove(jsonl)
        trainer.add_callback(JsonLoggerCallback(jsonl))
        
        # 2. HardwareMetricsCallback (m√©tricas de RAM, GPU, CPU, Disco)
        # Registra a cada 10 steps para n√£o sobrecarregar
        trainer.add_callback(HardwareMetricsCallback(output_dir, intervalo_steps=10))
        
        # 3. MetricsLoggerCallback (m√©tricas detalhadas de treinamento/valida√ß√£o)
        trainer.add_callback(MetricsLoggerCallback(output_dir))
        
        # 4. CheckpointRenameCallback (renomeia checkpoints com zero-padding)
        if self.save_checkpoints:
            chkpt_dir = os.path.join(self._yaml_config.modelo.saida, "chkpt")
            os.makedirs(chkpt_dir, exist_ok=True)
            trainer.add_callback(CheckpointRenameCallback(chkpt_dir))
        
        print(f' - callbacks de m√©tricas configurados:')
        print(f'   ‚Ä¢ metrics_stream.jsonl (m√©tricas brutas)')
        print(f'   ‚Ä¢ treinamento/hardware_metrics.jsonl (RAM, GPU, CPU, Disco)')
        print(f'   ‚Ä¢ treinamento/training_metrics.jsonl (loss, lr, eval_loss)')
        if self.save_checkpoints:
            print(f'   ‚Ä¢ checkpoint renaming (zero-padding: checkpoint-00001)')
        
        trainer.model.config.use_cache = False
        
        return trainer

    # ------------------------- checkpoint management --------------------- 
    def _find_latest_checkpoint(self) -> str:
        """Encontra o checkpoint mais recente na pasta de checkpoints.
        
        Returns:
            str: Caminho para o checkpoint mais recente ou None se n√£o houver
        """
        # verifica se o resume est√° habilitado na configura√ß√£o
        if not self._yaml_config.treinamento.resume_from_checkpoint:
            print("‚ö†Ô∏è Checkpoint ignorado por configura√ß√£o (resume_from_checkpoint=False)")
            return None
            
        if not self.save_checkpoints:
            return None
            
        checkpoint_dir = os.path.join(self._yaml_config.modelo.saida, "chkpt")
        if not os.path.exists(checkpoint_dir):
            return None
        
        # procura por pastas checkpoint-* 
        checkpoints = []
        for item in os.listdir(checkpoint_dir):
            item_path = os.path.join(checkpoint_dir, item)
            if os.path.isdir(item_path) and item.startswith("checkpoint-"):
                try:
                    step_num = int(item.split("-")[1])
                    checkpoints.append((step_num, item_path))
                except (IndexError, ValueError):
                    continue
        
        if not checkpoints:
            return None
            
        # retorna o checkpoint com maior n√∫mero de step
        latest_step, latest_path = max(checkpoints, key=lambda x: x[0])
        
        # verifica se √© um checkpoint v√°lido (cont√©m os arquivos necess√°rios)
        required_files = ["training_args.bin", "trainer_state.json"]
        alternative_files = ["model.safetensors"]  # formato alternativo
        
        has_required = all(os.path.exists(os.path.join(latest_path, f)) for f in required_files)
        has_alternative = any(os.path.exists(os.path.join(latest_path, f)) for f in alternative_files)
        
        if has_required or (has_alternative and os.path.exists(os.path.join(latest_path, "trainer_state.json"))):
            print(f"‚úÖ Checkpoint encontrado: {latest_path} (step {latest_step})")
            self.log_processamento(f"Checkpoint encontrado: {latest_path} (step {latest_step})", "CHECKPOINT_FOUND")
            return latest_path
        else:
            print(f"‚ö†Ô∏è  Checkpoint incompleto encontrado: {latest_path}")
            self.log_processamento(f"Checkpoint incompleto: {latest_path}", "CHECKPOINT_INCOMPLETE")
            return None

    # ------------------------- execu√ß√£o ----------------------------------
    def train(self):
        # Inicializa o trainer se necess√°rio (lazy init)
        if self.trainer is None:
            self.trainer = self._build_trainer()
            
        antes = _print_mem("ANTES")
        print("[4/6] Iniciando treinamento‚Ä¶")
        
        # Valida o modelo antes do treinamento
        print("\nüîç STATUS DO MODELO ANTES DO TREINAMENTO:")
        self.print_modelo_status()
        
        # verifica se existe checkpoint para continuar
        checkpoint_path = self._find_latest_checkpoint()
        resume_from_checkpoint = checkpoint_path is not None
        
        if resume_from_checkpoint:
            print(f"üîÑ Tentando continuar treinamento a partir do checkpoint: {checkpoint_path}")
            try:
                train_stats = self.trainer.train(resume_from_checkpoint=checkpoint_path)
                print("‚úÖ Treinamento continuado com sucesso a partir do checkpoint")
                self.log_processamento("Treinamento continuado com sucesso do checkpoint", "CHECKPOINT_SUCCESS")
            except Exception as e:
                error_msg = str(e)
                print(f"‚ùå Erro ao continuar do checkpoint: {error_msg}")
                print("üîÑ Reiniciando treinamento do in√≠cio...")
                
                # identifica tipos comuns de erro de checkpoint
                if any(keyword in error_msg.lower() for keyword in ['config', 'parameter', 'mismatch', 'size']):
                    self.log_processamento(f"Erro de configura√ß√£o no checkpoint: {error_msg}", "CHECKPOINT_CONFIG_ERROR")
                else:
                    self.log_processamento(f"Erro geral no checkpoint: {error_msg}", "CHECKPOINT_ERROR")
                
                # reinicia o treinamento do zero
                train_stats = self.trainer.train()
        else:
            print("üÜï Iniciando novo treinamento")
            train_stats = self.trainer.train()
            
        depois = _print_mem("DEPOIS")
        print("[5/6] Tempo de execu√ß√£o: {:.2f} s".format(train_stats.metrics["train_runtime"]))
        
        # Valida o modelo ap√≥s o treinamento
        print("\nüîç STATUS DO MODELO AP√ìS O TREINAMENTO:")
        info_modelo = self.print_modelo_status()
        
        # 2) dicion√°rio de tudo que interessa
        stats = {
            **train_stats.metrics,                  # train_loss, train_runtime, etc.
            "global_step":       train_stats.global_step,
            "training_loss":     train_stats.training_loss,
            "mem_gpu_before":    antes,
            "mem_gpu_after":     depois,
            "ds_train_len" : len(self.train_ds),
            "ds_eval_len" : len(self.eval_ds) if self.eval_ds else 0,
            "modelo_info": info_modelo,  # adiciona informa√ß√µes do modelo
        }
        
        # Gera relat√≥rio .md na pasta 'treinamento'
        try:
             hardware = Util.dados_hardware()
        except:
             hardware = {}
             
        gerador = GeradorRelatorio(self._yaml_config)
        gerador.gerar_relatorio(
            dataset_stats=self._dataset_stats,
            train_stats=stats,
            hardware_info=hardware
        )

        # grava o modelo antes do ultimo eval, pode dar erro de mem√≥ria no eval    
        self._save_model(stats=stats)
        # 3) garante um eval FINAL mesmo que j√° tenha havido evals em steps
        if self.eval_ds:
            final_eval = self.trainer.evaluate()     # roda avalia√ß√£o no eval_dataset
            stats.update(final_eval)                 # adiciona eval_loss, eval_runtime ‚Ä¶        self._save_model(stats = stats)
        
    # ------------------------- salvamento --------------------------------
    def _save_model(self, stats = None):
        out_dir = self._yaml_config.modelo.saida
        os.makedirs(out_dir, exist_ok=True)
        print(f"[6/6] Salvando modelo em {out_dir}‚Ä¶")
        
        # Salva o modelo (LoRA ou modelo completo)
        self.model.save_pretrained(out_dir)
        self.tokenizer.save_pretrained(out_dir)
        
        # Verifica se o modelo foi salvo corretamente
        adapter_config = os.path.join(out_dir, 'adapter_config.json')
        adapter_model = os.path.join(out_dir, 'adapter_model.safetensors')
        
        if os.path.exists(adapter_config):
            print(f"‚úÖ Arquivo de configura√ß√£o LoRA salvo: {adapter_config}")
            
        if os.path.exists(adapter_model):
            print(f"‚úÖ Modelo LoRA salvo: {adapter_model}")
        elif os.path.exists(os.path.join(out_dir, 'pytorch_model.bin')):
            print(f"‚úÖ Modelo PyTorch salvo: pytorch_model.bin")
        
        # Log detalhado do que foi salvo
        files_saved = []
        for file in os.listdir(out_dir):
            if file.endswith(('.json', '.safetensors', '.bin')):
                files_saved.append(file)
        
        self.log_processamento(f"Arquivos salvos em {out_dir}: {files_saved}", "MODEL_SAVED")
        
        if stats is not None:
            with open(os.path.join(self._yaml_config.modelo.saida, "metrics_summary.json"), "w") as fp:
                 json.dump(stats, fp, indent=2)
        print(r"Modelo salvo com sucesso \o/")

    def log_processamento(self, msg: str, titulo:str) -> None:
        ''' grava no arquivo de log com o nome _log_processamento_.txt dados importantes
            do processamento do treino como data, hora, par√¢metros, dataset, etc
        '''
        arquivo = os.path.join(self._yaml_config.modelo.saida, f"_log_processamento_.txt")
        with open(arquivo, "a") as f:
            _msg = f"{msg}" if isinstance(msg,str) else json.dumps(msg, indent=2, ensure_ascii=False)
            if titulo:
                f.write(f"\n{'='*60}\n[{datetime.now()}] {str(titulo).upper()}\n{'-'*60}\n{_msg}\n{'='*60}\n")
            else:
                f.write(f"[{datetime.now()}] {_msg}\n")

    def _place_inputs(self, inputs):
        try:
            target = self.model.device
        except AttributeError:
            target = next(self.model.parameters()).device
        return inputs.to(target)
    
    def prompt(self, texto: str, temperatura:int = 0, max_new_tokens: int = 2048, processador = None) -> Dict[str, Any]:
        """Tokeniza um prompt simples para teste r√°pido."""
        if not texto.strip():
            raise ValueError("Prompt vazio")
        
        # Verifica se o modelo tem LoRA ativo
        #is_peft_model = hasattr(self.model, 'peft_config') or hasattr(self.model, 'base_model')
        #model_type = type(self.model).__name__
        #print(f"üîç Tipo do modelo: {model_type} | PEFT ativo: {is_peft_model}")
        
        if callable(processador):
           inputs = processador(texto)
        else:
            # cria inst√¢ncia tempor√°ria do LLMsDataset para usar o m√©todo _process_single_message
            dataset_loader = LLMsDataset(
                path='',
                prompt_col='',
                tokenizer=self.tokenizer,
                max_seq_length=0
            )
            inputs = dataset_loader._process_single_message(messages=texto, 
                                                            inferencia=True, 
                                                            max_length=self._yaml_config.treinamento.max_seq_length)
        _temperatura = temperatura if isinstance(temperatura, (float, int)) else 0.2
        _temperatura = min(max(_temperatura,0.000001),1.0)
        #print(f'########### temperatura: {_temperatura}')
        # configura√ß√£o da predi√ß√£o
        gen_cfg = GenerationConfig.from_model_config(self.model.config)
        gen_cfg.max_new_tokens = max_new_tokens
        gen_cfg.min_length = 1
        gen_cfg.temperature = float(_temperatura)
        gen_cfg.top_k = 20 if _temperatura > 0.3 else 2
        gen_cfg.do_sample = bool(_temperatura > 0.3)
        # predi√ß√£o
        _inputs = self._place_inputs(inputs['input_ids'])
        _attention_mask = self._place_inputs(inputs.get('attention_mask', torch.ones_like(inputs['input_ids'])))
        input_length = _inputs.shape[1]  # comprimento da sequ√™ncia de entrada
        
        with torch.inference_mode():
             outputs = self.model.generate(_inputs, 
                                        attention_mask=_attention_mask,
                                        max_new_tokens=max_new_tokens,
                                        generation_config = gen_cfg)   
        # faz o decode s√≥ da resposta (remove os tokens de entrada)
        res = self.tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)       
        return {'texto': res, 'prompt_tokens': input_length, 'completion_tokens': len(outputs[0]) - input_length}

    def testar_predicoes(self, n_exemplos: int = 1, temperatura: float = 0.0, max_new_tokens: int = 2048, monitorar_memoria: bool = True) -> Dict[str, Any]:
        """
        Testa o modelo com exemplos do dataset de treino e exibe as predi√ß√µes.
        
        Args:
            n_exemplos: N√∫mero de exemplos para testar
            temperatura: Temperatura para gera√ß√£o
            max_new_tokens: N√∫mero m√°ximo de tokens a gerar
            monitorar_memoria: Se True, monitora RAM/GPU e gera gr√°fico
            
        Returns:
            Dict com resultados e m√©tricas de mem√≥ria (se monitorar_memoria=True)
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"üß™ TESTANDO MODELO COM {n_exemplos} EXEMPLO(S)")
        logger.info(f"{'='*80}")
        
        # Primeiro valida o status do modelo
        self.print_modelo_status()
        
        # verifica se h√° dataset dispon√≠vel
        if not hasattr(self, 'train_ds') or len(self.train_ds) == 0:
            logger.error("‚ùå Nenhum dataset de treino dispon√≠vel para teste")
            return {"erro": "Nenhum dataset dispon√≠vel"}
        
        # limita o n√∫mero de exemplos ao tamanho do dataset
        n_exemplos = min(n_exemplos, len(self.train_ds))
        
        # Inicia monitoramento de mem√≥ria se solicitado
        monitor = None
        if monitorar_memoria:
            monitor = MonitorRecursos(
                output_dir=self._yaml_config.modelo.saida,
                intervalo_segundos=0.5,
                nome_arquivo="memoria_predicao"
            )
            monitor.iniciar()
        
        resultados = []
        
        try:
            for i in range(n_exemplos):
                logger.info(f"\n{'-'*60}")
                logger.info(f">> EXEMPLO {i+1}/{n_exemplos}")
                logger.info(f"{'-'*60}")
                
                # pega o registro original do dataset
                if self._yaml_config.tipo_entrada == TIPO_ENTRADA_PASTAS:
                    # Modo pastas: recarrega mensagens em mem√≥ria
                    mensagens = self._yaml_config.dataset_manager.carregar_mensagens_de_pastas(alvo="treino")
                    
                    # Cria loader tempor√°rio com dados em mem√≥ria
                    dataset_loader = LLMsDataset(
                        data=mensagens,
                        tokenizer=self.tokenizer,
                        max_seq_length=self._yaml_config.treinamento.max_seq_length
                    )
                else:
                    # Modo dataset: recarrega do arquivo
                    dataset_loader = LLMsDataset(
                        path=self._yaml_config.dataset.train_file,
                        prompt_col=self._yaml_config.dataset.train_prompt_col,
                        tokenizer=self.tokenizer,
                        max_seq_length=self._yaml_config.treinamento.max_seq_length
                    )
                
                sample_row = dataset_loader.dataset[i]
                processador = lambda x: dataset_loader._process_single_message(x, max_length=self._yaml_config.treinamento.max_seq_length, inferencia=True)

                # detecta o formato e extrai prompt/completion esperado
                try:
                    if "messages" in sample_row:
                        messages = sample_row["messages"]
                        if isinstance(messages, list) and len(messages) >= 2:
                            user_msg = messages[0].get("content", "")
                            assistant_msg = messages[1].get("content", "")
                            if isinstance(user_msg, list):
                                user_msg = user_msg[0].get("text", "") if user_msg else ""
                            if isinstance(assistant_msg, list):
                                assistant_msg = assistant_msg[0].get("text", "") if assistant_msg else ""
                            prompt = user_msg
                            resposta_esperada = assistant_msg
                        else:
                            prompt = str(messages)
                            resposta_esperada = "N/A"
                    elif "prompt" in sample_row and "completion" in sample_row:
                        prompt = sample_row["prompt"]
                        resposta_esperada = sample_row["completion"]
                    else:
                        logger.error(f"‚ùå Formato de dados n√£o reconhecido para exemplo {i+1}")
                        continue
                    
                    logger.info(f">> PROMPT:")
                    if len(prompt) > 500:
                        logger.info(f"   {prompt[:250]} [...] {prompt[-250:]}")
                    else:
                        logger.info(f"   {prompt}")
                    
                    logger.info(f"\n>> RESPOSTA ESPERADA:")
                    if len(resposta_esperada) > 500:
                        logger.info(f"   {resposta_esperada[:250]} [...] {resposta_esperada[-250:]}")
                    else:
                        logger.info(f"   {resposta_esperada[:500]}{'...' if len(resposta_esperada) > 500 else ''}")
                    
                    # gera predi√ß√£o do modelo
                    try:
                        # Coleta m√©trica de mem√≥ria antes da predi√ß√£o
                        monitor_snapshot = monitor
                        if not monitor_snapshot:
                            # Cria inst√¢ncia tempor√°ria se monitoramento cont√≠nuo estiver desativado
                            monitor_snapshot = MonitorRecursos(self._yaml_config.modelo.saida)
                        
                        mem_antes = monitor_snapshot.coletar_metricas()
                        
                        tempo_inicio = time.time()
                        resultado = self.prompt(prompt, 
                                                temperatura=temperatura, 
                                                max_new_tokens=max_new_tokens,
                                                processador = processador)
                        tempo_predicao = time.time() - tempo_inicio
                        
                        # Coleta m√©trica de mem√≥ria depois da predi√ß√£o
                        mem_depois = monitor_snapshot.coletar_metricas()
                        
                        resposta_modelo = resultado['texto']
                        
                        logger.info(f"\n>> RESPOSTA DO MODELO:")

                        if len(resposta_modelo) > 500:
                            logger.info(f"   {resposta_modelo[:250]} [...] {resposta_modelo[-250:]}")
                        else:
                            logger.info(f"   {resposta_modelo}")
                        
                        logger.info(f"\n>> ESTAT√çSTICAS:")
                        logger.info(f"   - Tokens do prompt: {resultado.get('prompt_tokens', 'N/A')}")
                        logger.info(f"   - Tokens da resposta: {resultado.get('completion_tokens', 'N/A')}")
                        logger.info(f"   - Temperatura: {temperatura}")
                        logger.info(f"   - Tempo de predi√ß√£o: {tempo_predicao:.2f}s")
                        
                        # C√°lculo de Rouge-L
                        metricas_rouge, erro_rouge = calcular_rouge_l(resposta_esperada, resposta_modelo)
                        if metricas_rouge:
                             logger.info(f"   - Rouge-L True vs Pred: P={metricas_rouge['P']:.4f} R={metricas_rouge['R']:.4f} F1={metricas_rouge['F1']:.4f}")
                        elif erro_rouge:
                             # Warning simplificado
                             logger.warning(f"   - Rouge-L: {erro_rouge}")
                        
                        # Log de mem√≥ria
                        ram_diff = mem_depois.ram_usada_gb - mem_antes.ram_usada_gb
                        gpu_diff = mem_depois.gpu_usada_gb - mem_antes.gpu_usada_gb
                        logger.info(f"   - Mem√≥ria RAM: {mem_antes.ram_usada_gb:.1f}GB -> {mem_depois.ram_usada_gb:.1f}GB (delta: {ram_diff:+.1f}GB)")
                        logger.info(f"   - Mem√≥ria GPU: {mem_antes.gpu_usada_gb:.1f}GB -> {mem_depois.gpu_usada_gb:.1f}GB (delta: {gpu_diff:+.1f}GB)")
                        
                        resultados.append({
                            "exemplo": i + 1,
                            "prompt_tokens": resultado.get('prompt_tokens'),
                            "completion_tokens": resultado.get('completion_tokens'),
                            "tempo_segundos": round(tempo_predicao, 2),
                        })
                        
                    except Exception as e:
                        logger.error(f"‚ùå Erro ao gerar predi√ß√£o: {str(e)}\n{traceback.format_exc()}")
                        
                except Exception as e:
                    logger.error(f"‚ùå Erro ao processar exemplo {i+1}: {str(e)}")
        
        finally:
            # Para monitoramento e gera gr√°fico
            metricas_memoria = {}
            if monitor:
                metricas_memoria = monitor.parar()
                grafico_path = monitor.gerar_grafico()
                if grafico_path:
                    logger.info(f"üìà Gr√°fico de uso de mem√≥ria: {grafico_path}")
        
        logger.info(f"\n{'='*80}")
        logger.info(">> TESTE DE PREDI√á√ïES CONCLU√çDO")
        logger.info(f"{'='*80}")
        
        return {
            "resultados": resultados,
            "n_exemplos": len(resultados),
            "metricas_memoria": metricas_memoria,
        }

    def validar_modelo_lora(self) -> dict:
        """Valida se o modelo LoRA est√° carregado corretamente e retorna informa√ß√µes detalhadas."""
        info = {
            'modelo_tipo': type(self.model).__name__,
            'is_peft_model': False,
            'adapters_ativos': [],
            'parametros_treinaveis': 0,
            'parametros_totais': 0,
            'lora_detectado': False
        }
        
        # Verifica se √© modelo PEFT
        info['is_peft_model'] = hasattr(self.model, 'peft_config') or hasattr(self.model, 'base_model')
        
        if info['is_peft_model']:
            info['lora_detectado'] = True
            
            # Obt√©m informa√ß√µes dos adaptadores
            if hasattr(self.model, 'peft_config'):
                peft_configs = self.model.peft_config
                for adapter_name, config in peft_configs.items():
                    adapter_info = {
                        'nome': adapter_name,
                        'r': getattr(config, 'r', 'N/A'),
                        'alpha': getattr(config, 'lora_alpha', 'N/A'),
                        'dropout': getattr(config, 'lora_dropout', 'N/A'),
                        'target_modules': getattr(config, 'target_modules', [])
                    }
                    info['adapters_ativos'].append(adapter_info)
        
        # Conta par√¢metros trein√°veis
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        info['parametros_treinaveis'] = trainable_params
        info['parametros_totais'] = total_params
        info['percentual_treinavel'] = (trainable_params / total_params * 100) if total_params > 0 else 0
        
        return info

    def print_modelo_status(self):
        """Imprime o status detalhado do modelo."""
        info = self.validar_modelo_lora()
        
        print(f"\n{'='*60}")
        print(f"üìä STATUS DETALHADO DO MODELO")
        print(f"{'='*60}")
        print(f"Tipo do modelo: {info['modelo_tipo']}")
        print(f"√â modelo PEFT: {info['is_peft_model']}")
        print(f"LoRA detectado: {info['lora_detectado']}")
        print(f"Par√¢metros trein√°veis: {info['parametros_treinaveis']:,}")
        print(f"Par√¢metros totais: {info['parametros_totais']:,}")
        print(f"Percentual trein√°vel: {info['percentual_treinavel']:.4f}%")
        
        if info['adapters_ativos']:
            print(f"\nüîß ADAPTADORES ATIVOS:")
            for adapter in info['adapters_ativos']:
                print(f"  - {adapter['nome']}: r={adapter['r']}, alpha={adapter['alpha']}")
                modules = adapter['target_modules']
                if isinstance(modules, str) and len(modules) > 50 and modules.startswith("(?:"):
                     modules_str = "Unsloth Default (Todos os m√≥dulos lineares)"
                else:
                     modules_str = str(modules)
                print(f"    Modules: {modules_str}")
        else:
            print(f"\n‚ö†Ô∏è  NENHUM ADAPTADOR ATIVO DETECTADO")
        
        print(f"{'='*60}")
        
        # Log no arquivo
        self.log_processamento(info, "STATUS_MODELO_DETALHADO")
        
        return info

# ---------------------------------------------------------------------------
# Datasets
# ---------------------------------------------------------------------------

class LLMsDataset:
    """Gerencia datasets para fine‚Äëtuning de LLMs com Unsloth."""

    def __init__(self, path: str = None, prompt_col: str = None, tokenizer=None, 
                 max_seq_length: int = 4096, data: list = None):
        """
        Inicializa o dataset a partir de arquivo ou dados em mem√≥ria.
        
        Args:
            path: Caminho do arquivo (parquet/json/jsonl/txt)
            prompt_col: Nome da coluna com prompts (para parquet)
            tokenizer: Tokenizer do modelo
            max_seq_length: Tamanho m√°ximo da sequ√™ncia
            data: Dados j√° carregados em mem√≥ria (lista de dicts com 'messages')
        """
        self.path = path
        self.prompt_col = prompt_col
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        self._template_com_type = self.template_com_type(tokenizer) if tokenizer else False
        
        # Carrega de dados em mem√≥ria ou de arquivo
        if data is not None:
            # Dados j√° carregados (do YamlTreinamento.carregar_mensagens_de_pastas)
            self.dataset = Dataset.from_list(data)
        elif path and os.path.isfile(path):
            self.dataset = self._load_dataset()
        else:
            # Dataset vazio (para preparar apenas para predi√ß√£o)
            self.dataset = Dataset.from_list([])

    @staticmethod
    def template_com_type(tokenizer) -> bool:
        """Identifica se o tokenizer usa format type (lista) ou string no content."""
        msgs_list = [{"role": "user", "content": [{"type":"text","text":"ping"}]}]
        msgs_str  = [{"role": "user", "content": "ping"}]
        try:
            tokenizer.apply_chat_template(msgs_list, tokenize=False)
            return  True # lista de partes funcionou
        except Exception:
            pass
        # se lista falhou, tente string
        tokenizer.apply_chat_template(msgs_str, tokenize=False)  # lan√ßa se n√£o suportar
        return False # string funcionou

    def _load_dataset(self):
        """Carrega dataset de arquivo parquet, json, jsonl ou txt."""
        ext = os.path.splitext(self.path)[1].lower()
        if ext == ".parquet":
            df = pd.read_parquet(self.path)
            if self.prompt_col and self.prompt_col not in df.columns:
                raise KeyError(f"Coluna '{self.prompt_col}' n√£o encontrada em {self.path}")
            return Dataset.from_pandas(df)
        elif ext in {".json", ".jsonl", ".txt"}:
            self.prompt_col = None  # json n√£o precisa de coluna
            # tenta utf-8, se falhar tenta latin-1
            try:
                dados = open(self.path, "r", encoding="utf-8").readlines()
            except UnicodeEncodeError:
                dados = open(self.path, "r", encoding="latin-1").readlines()
            registros = [json.loads(linha) for linha in dados if linha.strip()]
            return Dataset.from_list(registros)
        else:
            raise ValueError(f"Formato de arquivo n√£o suportado: {ext}")

    def _detect_dataset_format(self, sample_row) -> str:
        """Detecta o formato do dataset analisando uma linha de exemplo.
        
        Returns:
            'messages': se tem coluna messages com lista de mensagens user/assistant
            'prompt_completion': se tem colunas prompt e completion
        """
        if isinstance(sample_row, dict):
            if "messages" in sample_row:
                return "messages"
            elif "prompt" in sample_row and "completion" in sample_row:
                return "prompt_completion"
        raise ValueError(f"Formato de dataset n√£o reconhecido: {list(sample_row.keys()) if isinstance(sample_row, dict) else type(sample_row)}")

    def _process_single_message(self, messages, max_length: int = None, inferencia: bool = False) -> Dict[str, Any]:
        """Processa uma √∫nica mensagem ou par prompt/completion usando chat template.
        
        Args:
            messages: Mensagens no formato dict ou lista de mensagens user/assistant
            max_length: Comprimento m√°ximo da sequ√™ncia (usa self.max_seq_length se None)
            
        Returns:
            Dict com input_ids, attention_mask e labels prontos para treinamento
        """
        if max_length is None:
            max_length = self.max_seq_length
            
        # se for um dict com coluna prompt e completion, transforma em lista
        if isinstance(messages, str):
            prompt = messages if not self._template_com_type else [{"type": "text", "text": messages}]
            _messages = [{"role": "user", "content": prompt}]
        elif isinstance(messages, dict) and "prompt" in messages and "completion" in messages:
            prompt = messages["prompt"] if not self._template_com_type else [{"type": "text", "text": messages["prompt"]}]
            completion = messages["completion"] if not self._template_com_type else [{"type": "text", "text": messages["completion"]}]
            _messages = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": completion},
            ]
        else:
            # confere user assistant e estrutura
            if not (isinstance(messages, (list, np.ndarray, tuple)) and len(messages) == 2):
                raise ValueError(f"Esperado lista de 2 mensagens user/assistant >> {type(messages)}")
            if messages[0].get("role") != "user" or messages[1].get("role") != "assistant":
                raise ValueError("Ordem user/assistant inv√°lida")
            _messages = deepcopy(messages)
            for message in _messages:
                if ("content" not in message) or ("role" not in message):
                    raise ValueError("Cada mensagem deve ter uma chave 'content' e 'role'.")
                if self._template_com_type:
                    if isinstance(message["content"], str):
                        message["content"] = [{"type": "text", "text": message["content"]}]
                else:
                    if not isinstance(message["content"], str):
                        raise ValueError("O campo 'content' deve ser uma string.")
        
        # identifica o que √© prompt para ajustar attention mask
        prompt_ids = self.tokenizer.apply_chat_template(
                _messages[:1],   # s√≥ a primeira mensagem
                tokenize=True,
                add_generation_prompt=True,   # adiciona o cabe√ßalho do assistant/model
                return_tensors='pt' if inferencia else None,
            )        
        if inferencia:
            # retorna lista de listas que √© o esperado pelo generate
            attention_mask = torch.ones_like(prompt_ids)
            return {"input_ids": prompt_ids, "attention_mask": attention_mask}
        # processa tudo para ajustar attention mask
        full_ids = self.tokenizer.apply_chat_template(
                _messages,
                tokenize=True,
                add_generation_prompt=False,
                return_tensors=None,
            )
        
        #  uma mensagem
        if isinstance(full_ids[0], (list, tuple, np.ndarray)):
            full_ids = full_ids[0]   # extrai a primeira sublista
        if isinstance(prompt_ids[0], (list, tuple, np.ndarray)):
            prompt_ids = prompt_ids[0]  # extrai a primeira sublista
            
        # Converte para lista se for NumPy array para evitar problemas de concatena√ß√£o
        if isinstance(full_ids, np.ndarray):
            full_ids = full_ids.tolist()
        if isinstance(prompt_ids, np.ndarray):
            prompt_ids = prompt_ids.tolist()
            
        # truncando max_length e aplicando mascara para processar apenas a resposta
        input_ids = full_ids[:max_length]
        attention_mask = [1] * len(input_ids)
        if len(input_ids) < max_length:
            pad_len = max_length - len(input_ids)
            input_ids = input_ids + [self.tokenizer.pad_token_id] * pad_len
            attention_mask = attention_mask + [0] * pad_len

        # 4) Cutoff = in√≠cio da resposta do assistant (limitado pelo max_length)
        cutoff = min(len(prompt_ids), max_length)

        # 5) Labels = c√≥pia de input_ids; m√°scara em [0:cutoff) e em padding
        labels = input_ids.copy()
        for j in range(max_length):
            if j < cutoff or attention_mask[j] == 0:
                labels[j] = -100 # labels para serem ignorados no treinamento

        # Garante que todos sejam listas de inteiros (n√£o arrays NumPy)
        return {
            "input_ids": [int(x) for x in input_ids],
            "attention_mask": [int(x) for x in attention_mask],
            "labels": [int(x) for x in labels],
        }

    def preprocess_with_chat_template(self, dataset: Dataset = None, max_length: int = None) -> Dataset:
        """Processa um dataset completo usando chat template e retorna tokens prontos para treinamento.
        
        Args:
            dataset: Dataset a ser processado (usa self.dataset se None)
            max_length: Comprimento m√°ximo da sequ√™ncia (usa self.max_seq_length se None)
            
        Returns:
            Dataset processado com input_ids, attention_mask e labels
        """
        if dataset is None:
            dataset = self.dataset
            
        if max_length is None:
            max_length = self.max_seq_length
            
        # detecta o formato do dataset analisando o primeiro registro
        sample_row = dataset[0]
        dataset_format = self._detect_dataset_format(sample_row)
        print(f">> Formato detectado: {dataset_format}")
        
        def process_row(row):
            if dataset_format == "messages":
                # dataset com coluna messages
                messages = row[self.prompt_col] if self.prompt_col else row["messages"]
            else:  # prompt_completion
                # dataset com colunas prompt e completion
                messages = {
                    "prompt": row["prompt"],
                    "completion": row["completion"]
                }
            
            return self._process_single_message(messages, max_length)
        
        # aplica processamento a todo o dataset
        processed_dataset = dataset.map(process_row, remove_columns=dataset.column_names)
        print(f">> Dataset processado: {len(processed_dataset)} registros")
        
        return processed_dataset

    def get_processed_dataset(self) -> Dataset:
        """Retorna dataset processado com chat template aplicado."""
        return self.preprocess_with_chat_template()
    
    def print_sample(self, n: int = 1) -> None:
        """Imprime exemplo de registro do dataset processado."""
        print(f'Exemplo de registro do dataset [n={n}]:')
        print(json.dumps(self.get_sample(n), indent=2, ensure_ascii=False))

    def get_sample(self, n: int = 1) -> dict:
        """Retorna registro do dataset com texto decodificado para an√°lise."""
        # detecta formato do primeiro registro
        sample_row = self.dataset[0]
        dataset_format = self._detect_dataset_format(sample_row)
        
        if n == 1:
            row = self.dataset[0]
            if dataset_format == "messages":
                messages = row[self.prompt_col] if self.prompt_col else row["messages"]
            else:  # prompt_completion
                messages = {"prompt": row["prompt"], "completion": row["completion"]}
            
            # processa a mensagem
            result = self._process_single_message(messages)
            # result['input_ids'] j√° √© uma lista de inteiros, n√£o precisa de .tolist()
            texto = self.tokenizer.decode(result['input_ids'], skip_special_tokens=False)
            result['texto_decodificado'] = texto
        else:
            # para m√∫ltiplas amostras
            result = {'input_ids': [], 'attention_mask': [], 'labels': [], 'texto_decodificado': []}
            for i in range(min(n, len(self.dataset))):
                row = self.dataset[i]
                if dataset_format == "messages":
                    messages = row[self.prompt_col] if self.prompt_col else row["messages"]
                else:  # prompt_completion
                    messages = {"prompt": row["prompt"], "completion": row["completion"]}
                
                sample_result = self._process_single_message(messages)
                result['input_ids'].append(sample_result['input_ids'])
                result['attention_mask'].append(sample_result['attention_mask'])
                result['labels'].append(sample_result['labels'])
                
                # sample_result['input_ids'] j√° √© uma lista de inteiros
                texto = self.tokenizer.decode(sample_result['input_ids'], skip_special_tokens=False)
                result['texto_decodificado'].append(texto)
        
        return result

    def get_stats(self) -> dict:
        """Retorna estat√≠sticas b√°sicas do dataset."""
        # detecta formato do dataset
        sample_row = self.dataset[0]
        dataset_format = self._detect_dataset_format(sample_row)
        
        return {
            "total_registros": len(self.dataset),
            "colunas": list(self.dataset.column_names),
            "formato_arquivo": os.path.splitext(self.path)[1].lower(),
            "formato_dataset": dataset_format,
            "caminho": self.path,
            "coluna_prompt": self.prompt_col,
            "max_seq_length": self.max_seq_length,
            "template_com_type": self._template_com_type,
        }

# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def _create_default_cfg(path: str) -> None:
    template = {
        "formatos": {
            "tipo_entrada": "dataset",
            "formato_saida": "texto",
        },
        "misc": {
            "log_level": "INFO",
            "env_chave_criptografia": "",  # Nome da vari√°vel de ambiente com chave Fernet
        },
        "dataset": {
            "train_file": "../dataset/data/dados_unificados_sm_treino.parquet",
            "train_prompt_col": "messages",
            "eval_file": "",
            "eval_prompt_col": "",
        },
        "modelo": {
            "base_model_name": "unsloth/gemma-3-12b-it-unsloth-bnb-4bit",
            "saida": "../modelos/gemma-3-12b-refleg20k-v01",
        },
        "treinamento": {
            "eval_steps": "15%",
            "batch_size": 2,
            "grad_batch_size": 5,
            "num_train_epochs": 1,
            "max_seq_length": 4096,
            "learning_rate": 2e-4,
            "save_checkpoints": True,
            "resume_from_checkpoint": True,
            "warmup_steps": 5,
            "nbits": 4,
            "train_on_responses_only": True,
        },
        "lora": {
            "r": 8,
            "alpha": 32,
            "dropout": 0.05,
            "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj",
                           "gate_proj", "up_proj", "down_proj"],
        }
    }
    with open(path, "w", encoding="utf-8") as fp:
        yaml.safe_dump(template, fp, sort_keys=False, allow_unicode=True)


def _verificar_modelo_treinado(yaml_config: YamlTreinamento) -> bool:
    """
    Verifica se existe modelo LoRA treinado na pasta de sa√≠da.
    
    Returns:
        True se existir modelo treinado, False caso contr√°rio
    """
    output_dir = yaml_config.modelo.saida
    arq_lora = os.path.join(output_dir, 'adapter_config.json')
    arq_model = os.path.join(output_dir, 'adapter_model.safetensors')
    arq_pytorch = os.path.join(output_dir, 'pytorch_model.bin')
    
    return os.path.exists(arq_lora) and (os.path.exists(arq_model) or os.path.exists(arq_pytorch))


def _perguntar_usar_modelo_base() -> bool:
    """
    Pergunta ao usu√°rio se deseja usar o modelo base para predi√ß√£o.
    
    Returns:
        True para continuar com modelo base, False para cancelar
    """
    logger.warning("\n‚ö†Ô∏è  ATEN√á√ÉO: N√£o foi encontrado modelo LoRA treinado na pasta de sa√≠da.")
    logger.info("O modelo base ser√° carregado para predi√ß√£o (sem fine-tuning).\n")
    
    try:
        resposta = input("Deseja continuar com o modelo base? [s/N]: ").strip().lower()
        return resposta in ('s', 'sim', 'y', 'yes')
    except (KeyboardInterrupt, EOFError):
        logger.info("\nOpera√ß√£o cancelada pelo usu√°rio.")
        return False


def _cli() -> None:
    parser = argparse.ArgumentParser(
        description="Fine-tune LLMs (Gemma, Qwen, Llama, DeepSeek) com configura√ß√£o YAML.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
A√ß√µes dispon√≠veis:
  --info            Informa√ß√µes gerais do treinamento e modelo
  --stats           Relat√≥rio estat√≠stico com tokens de entrada/sa√≠da e boxplots
  --treinar         Inicia ou reinicia o treinamento
  --reset           Limpa o treinamento atual (com confirma√ß√£o)
  --predict         Gera predi√ß√µes para todos os subsets (treino, validacao, teste)
  --predict-treino  Gera predi√ß√µes apenas do subset de treino
  --predict-validacao  Gera predi√ß√µes apenas do subset de valida√ß√£o
  --predict-teste   Gera predi√ß√µes apenas do subset de teste
  
Sem a√ß√£o: modo interativo que pergunta qual a√ß√£o executar.

Exemplos:
  %(prog)s config.yaml              # Modo interativo
  %(prog)s config.yaml --info       # Informa√ß√µes detalhadas
  %(prog)s config.yaml --treinar    # Inicia treinamento
  %(prog)s config.yaml --reset --treinar  # Limpa e treina
  %(prog)s config.yaml --predict    # Gera predi√ß√µes de todos os subsets
  %(prog)s config.yaml --modelo 5   # Testa 5 predi√ß√µes interativas
"""
    )
    parser.add_argument("config", help="Arquivo YAML com as configura√ß√µes.")
    
    # Grupo de a√ß√µes (mutuamente exclusivas, exceto reset+treinar)
    parser.add_argument("--info", action="store_true", 
                        help="Modo info: exibe estrutura do dataset e configura√ß√£o sem treinar")
    parser.add_argument("--stats", action="store_true",
                        help="Gera relat√≥rio estat√≠stico de tokens com boxplots")
    parser.add_argument("--treinar", action="store_true",
                        help="Inicia ou continua o treinamento")
    parser.add_argument("--reset", action="store_true",
                        help="Limpa treinamento atual (checkpoints e modelo LoRA)")
    
    # A√ß√µes de predi√ß√£o
    parser.add_argument("--predict", action="store_true",
                        help="Gera predi√ß√µes para todos os subsets (treino, validacao, teste)")
    parser.add_argument("--predict-treino", action="store_true",
                        help="Gera predi√ß√µes apenas do subset de treino")
    parser.add_argument("--predict-validacao", action="store_true",
                        help="Gera predi√ß√µes apenas do subset de valida√ß√£o")
    parser.add_argument("--predict-teste", action="store_true",
                        help="Gera predi√ß√µes apenas do subset de teste")
    
    # A√ß√£o de merge
    parser.add_argument("--merge", action="store_true",
                        help="Exporta e realiza merge do modelo treinado com o base")
    parser.add_argument("--quant", type=str, default=None,
                        help="M√©todo de quantiza√ß√£o para merge: 16bit, 4bit, q4_k_m, q8_0, f16 (padr√£o: interativo ou 16bit)")
    
    # Op√ß√µes modificadoras
    parser.add_argument("--base", action="store_true",
                        help="For√ßa o uso do modelo base (ignora LoRA treinado) para info e predict")
    
    # Op√ß√µes extras (mantidas do CLI anterior)
    parser.add_argument("--modelo", type=int, nargs='?', const=1, 
                        help="Modo teste: exibe exemplos de prompt e resposta do modelo treinado (padr√£o: 1 exemplo)")
    parser.add_argument("--log-level", type=str, default=None, 
                        choices=["DEBUG", "INFO", "WARNING", "ERROR"], 
                        help="N√≠vel de log (sobrescreve misc.log_level do YAML)")
    

    # Retrocompatibilidade: mant√©m --debug como alias para --info
    parser.add_argument("--debug", action="store_true", 
                        help=argparse.SUPPRESS)  # Oculto, usa --info

    # Inje√ß√£o de dicas
    parser.add_argument("--dicas", action="store_true", 
                        help="Injeta coment√°rios de dicas no YAML de configura√ß√£o")

    
    args = parser.parse_args()

    cfg_path = args.config or "./exemplo.yaml"
    
    # Carrega configura√ß√£o do YAML para determinar log_level padr√£o
    log_level_padrao = "INFO"
    if os.path.exists(cfg_path):
        try:
            yaml_config = YamlTreinamento(cfg_path, validar_caminhos=False)
            log_level_padrao = yaml_config.misc.log_level
        except Exception:
            pass  # Usa padr√£o INFO se falhar
    

    # Log level configuration...
    nivel_log = args.log_level if args.log_level else log_level_padrao
    configurar_logging(nivel=nivel_log)
    
    logger.debug(f"Log level configurado: {nivel_log} (CLI: {args.log_level}, YAML: {log_level_padrao})")


    # Inje√ß√£o de dicas antes de qualquer valida√ß√£o de CUDA ou cria√ß√£o de template
    if args.dicas:
        from treinar_unsloth_actions import executar_injetar_dicas
        executar_injetar_dicas(cfg_path)

    # informa√ß√µes sobre CUDA

    if torch.cuda.is_available():
        logger.info(f"CUDA dispon√≠vel ‚Äî {torch.cuda.device_count()} GPU(s) detectada(s)")
    else:
        logger.warning("CUDA n√£o dispon√≠vel ‚Äî treinamento ser√° na CPU (muito mais lento)")

    # Cria YAML template se n√£o existir
    if not os.path.exists(cfg_path):
        _create_default_cfg(cfg_path)
        logger.info(
            f"Arquivo de configura√ß√£o criado em '{cfg_path}'.\n"
            "Revise os par√¢metros e execute novamente para iniciar o treinamento."
        )
        sys.exit(0)

    # Importa m√≥dulo de a√ß√µes
    from treinar_unsloth_actions import (
        executar_info, executar_stats, executar_treinar, 
        executar_reset, executar_predict, executar_merge, modo_interativo, executar_acao
    )
    
    # Retrocompatibilidade: --debug -> --info
    if args.debug:
        args.info = True
        logger.warning("‚ö†Ô∏è  O par√¢metro --debug est√° deprecado. Use --info.")



    # Modo --modelo: teste de predi√ß√µes (fluxo separado)
    if args.modelo:
        # Carrega apenas a configura√ß√£o YAML para verificar modelo
        yaml_config = YamlTreinamento(cfg_path, validar_caminhos=False)
        
        usar_base = getattr(args, 'base', False)
        
        if not usar_base:
            if not _verificar_modelo_treinado(yaml_config):
                if not _perguntar_usar_modelo_base():
                    logger.info("Opera√ß√£o cancelada. Execute um treinamento antes de testar o modelo.")
                    sys.exit(0)
                else:
                    logger.info("Continuando com modelo base (sem fine-tuning)...\n")
                    usar_base = True # Usu√°rio confirmou usar base na pergunta
            else:
                logger.info(f"‚úÖ Modelo LoRA treinado encontrado em: {yaml_config.modelo.saida}")
        else:
            logger.info("‚ÑπÔ∏è  Op√ß√£o --base ativada: For√ßando uso do modelo base.")

        trainer = LLMsTrainer(cfg_path, force_base=usar_base)
        n_exemplos = args.modelo if isinstance(args.modelo, int) else 1
        resultado = trainer.testar_predicoes(n_exemplos=n_exemplos, temperatura=0.2, max_new_tokens=512)
        
        # Exibe resumo das m√©tricas de mem√≥ria
        if resultado.get('metricas_memoria'):
            metricas = resultado['metricas_memoria']
            logger.info("\nüìä RESUMO DE USO DE MEM√ìRIA:")
            if 'ram' in metricas:
                logger.info(f"   RAM: m√°x={metricas['ram'].get('max_gb', 0):.1f} GB, m√©dia={metricas['ram'].get('media_gb', 0):.1f} GB")
            if 'gpu' in metricas and metricas['gpu'].get('num_gpus', 0) > 0:
                logger.info(f"   GPU: m√°x={metricas['gpu'].get('max_gb', 0):.1f} GB, m√©dia={metricas['gpu'].get('media_gb', 0):.1f} GB ({metricas['gpu'].get('num_gpus', 0)} GPU(s))")
        
        sys.exit(0)

    # Determina subsets para predict (se houver)
    predict_subsets = None
    has_predict = False
    
    if getattr(args, 'predict_treino', False):
        predict_subsets = predict_subsets or []
        predict_subsets.append('treino')
        has_predict = True
    if getattr(args, 'predict_validacao', False):
        predict_subsets = predict_subsets or []
        predict_subsets.append('validacao')
        has_predict = True
    if getattr(args, 'predict_teste', False):
        predict_subsets = predict_subsets or []
        predict_subsets.append('teste')
        has_predict = True
    if args.predict:
        has_predict = True
        # predict_subsets = None significa todos
    
    # Determina a√ß√£o a executar
    if args.info:
        executar_info(cfg_path)
    elif args.stats:
        executar_stats(cfg_path)
    elif args.merge:
        executar_merge(cfg_path, quantizacao=args.quant)
    else:
        executed_action = False
        
        # 1. Executa Treinamento (com ou sem Reset)
        if args.treinar:
            executar_treinar(cfg_path, reset=args.reset)
            executed_action = True
        # 2. Se n√£o treinou mas pediu reset
        elif args.reset:
            executar_reset(cfg_path, confirmar=True)
            executed_action = True
            
        # 3. Executa Predi√ß√£o (pode ser executado ap√≥s treino ou reset)
        if has_predict:
            executar_predict(cfg_path, subsets=predict_subsets, usar_base=args.base)
            executed_action = True
            
        # 4. Se nenhuma a√ß√£o foi executada via CLI, entra no modo interativo
        if not executed_action:
            acao = modo_interativo(cfg_path)
            if acao:
                executar_acao(acao, cfg_path)



if __name__ == "__main__":
    # Carrega .env do diret√≥rio src (funciona de qualquer pasta)
    UtilEnv.carregar_env(pastas=[_SRC_DIR, './', '../', '../src/'])
    _cli()

